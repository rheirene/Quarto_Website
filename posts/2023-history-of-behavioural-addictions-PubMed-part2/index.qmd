---
title: "The history of behavioural addictions research (according to PubMed): Part 2"
description: ""
author:
  - name: Rob Heirene
categories: [Gambling] 
date: 2023-08-25
draft: true 
---

### Data & code set-up

Just like in Part 1, I'll first load all the required packages and fonts for figures.

```{r results=FALSE, warning=FALSE, message=FALSE}
#| code-fold: true
#| code-summary: "Code: set-up"

# Install and load the groundhog package to ensure consistency of the package versions used here:
# install.packages("groundhog") # Install

library(groundhog) # Load

# List desired packages:
packages <- c('readr', # Load dataset from GitHib
              'tidyverse', # Clean, organise, and visualise data
              'gt', #  table data
              'gtExtras', # Add colours to gt tables
              'plotly', # Add interactive elements to figures
              'gganimate', # Make animated plots
              'transformr', # Needed for certain animations (dumbell lines)
              'png',# Helps render gganimate plots
              'gifski', # Helps render gganimate plots
              'rmarkdown', # Helps render gganimate plots
              'av', # render gganimate plots as videos
              'Cairo', # Anti-aliasing for the line plots (smoothing output)
              'ggtext', # make fancy labels in plots
              'sysfonts', # Special fonts for figures
              'showtext', # Special fonts for figures
              'htmlwidgets', # Make plotly plots HTML format for rendering in Quarto
              'scico', # Colour palette
              'maps', # Get map/geographic data for author locations
              'stringr', # extract city names
              'sessioninfo') # Detailed session info for reproducibility 

# Load desired package with versions specific to project start date:
groundhog.library(packages, "2023-08-01") 

# Load new font for figures/graphs
font_add_google("Poppins", "Poppins")
font_add_google("Reem Kufi", "Reem Kufi")
showtext_auto(enable = TRUE) 


plot_theme <-  theme(plot.background = element_rect(fill = "#002B36",  color = NA), # ADDING THIS NA REMOVES BORDER AROUND PLOT ON WEBSITE
     panel.background = element_rect(fill = "#002B36"),
     text = element_text(family = "Reem Kufi", color = "#F5F7F0"),
     axis.text = element_text(color = "#F5F7F0", size = 13),
     panel.grid = element_blank(),
     plot.title = element_text(color = "#F5F7F0", size = 16),
     plot.subtitle = element_text(color = "#50B5C8"),
     plot.caption = element_text(color = "#50B5C8"))
```

Now I'll load in the dataset and do a little cleaning. Again,, I'm going to remove all publications from 2023 so that we only have data for complete years (see comments in the code chunk below for any other exclusions).

```{r results=FALSE, warning=FALSE, message=FALSE}
#| code-fold: true
#| code-summary: "Code: load dataset"

url_behav_addic_data_link <- "https://raw.githubusercontent.com/rheirene/pub-med-scape-behav-addictions/main/Data%20extraction/combined_results_clean.csv"

raw_data <- read_csv(url_behav_addic_data_link) %>%
  as_tibble()

str(raw_data)
# View(data)

# Despite my best efforts with manual searching, my explorations of this dataset in R revealed that there are a few erratums/corrigendums and one notice of retraction included in the data. Let's remove these before moving forward:
filtered_data <- raw_data %>%
  filter(str_detect(Publication_Type, "Erratum") | 
         str_detect(Publication_Type, "corrigendum") | 
         str_detect(Publication_Type, "Retraction")) %>% 
  distinct(PMID, .keep_all = TRUE)

# Let's now remove these pubs and any from 2023 so we have data for all "full" years:
data <- raw_data %>% 
  anti_join(filtered_data) %>%
    filter(Year != "2023") 

# View(data)

```

### Where do these papers come from?

I want to to visualise this information, but it's going to be quite tricky as each paper has a variable number of authors and therefore institution addresses, all of which are listed in a single (often messy) string within one column in the dataset. I'll have to separate out each author institution and then find a way to extract only the relevant information to be able to geo-locate them.

```{r}
#| code-fold: true
#| code-summary: "Code: Extracy location data for each paper"

# ***********************The below code is almost all commented out on purpose as the process of extracting and matching city names from the author address column is so computationally taxing that it takes a long time to process.  I've left the code here so that anyone can see how I did it, but I saved the results as a .csv file and now load the data like that***********************
# as_tibble(data$Author_Address) # Take a look at how the author addresses are structured

# Okay, so we're going to need to create an ID variable for each paper (this makes the string split and a nest below work bettter than relying on titles), Then split the author_address strings into separate addresses, then unnest these into new rows. 

# The un-nest doesn't seem to work well when we retain all of the columns in the dataset, so I do it with only id and institution (address) in the data, then join all of the rest of the data set to the unnested rows after this. For this to work, we need to create a dataset that has the ID variable in before splitting the string and and unnesting. Let's do that:
#  data_id <-data %>%
#   rowid_to_column(var = "id")
# 
# # Now split the address  string and then  unnest it into multiple rows, and finally re-join with the main dataset
# data_locations <- data_id %>%
#   mutate(institution = str_split(Author_Address, ";")) %>%
#   select(id, institution) %>%
#   unnest(institution) %>%
#   full_join(data_id, by = "id") %>%
#   # Whilst we're doing this, we'll also create a counter/number for each institution per paper
#   group_by(id) %>%
#   mutate(author_num = row_number()) %>%
#   ungroup()
#   # We can also pivot to wide format if that makes sense at any point:
#   # pivot_wider(names_from = author_num,
#   #             values_from = institution,
#   #             names_prefix = "author_",
#   #             values_fill = NA_character_)
# 
# # View(data_locations) # Looks good!
# 
# # Fortunately, the "maps"  package contains a list of city names that we can use to match with our author institutions. Let's load the relevant data:
# ## Loading country data from package maps
# data(world.cities)
#  
# world_cities_filtered <-  world.cities %>%
#   group_by(name) %>%
#   filter(pop == max(pop)) %>% # Okay, so this is imperfect, but when a city name is duplicated, this will filter to select only the one with the highest population. This is based on the assumption that papers are likely to come from more populated cities (i.e. those with universities). This may seem crude, but it solved many, many issues in the map with, for example, putting "Cambridge" in the Caribbean as one of the most actively publishing cities on behavioural addictions in the world....
#   ungroup() %>% 
#   filter(!name %in% c("China", # There is a city in Mexico called China, and including this in the dataset needed to pick up any papers published in China and link them to this city!
#                         "India", # Same sort of issue (City in Africa)
#                         "San",  # Same sort of issue (City in Africa, again)
#                         "Institut" # This appears to be act somewhere around Azerbaijan, but I think it's getting picked up as a city when in fact it just is a string in the author address referring to a university!
#                        ))
# 
# 
# # Extract just the city names so we can try and match author locations using these:
# city_names_from_world <- world_cities_filtered$name
# 
# # Create a pattern of city names for matching with word boundaries:
# city_names_pattern <- paste("\\b(", paste(city_names_from_world, collapse = "|"), ")\\b", sep = "")
# 
# # Extract city namesusing stringr (the str_exact function extracts the first complete match from a string; the arguments are the string and then the match you're looking for)
# cities_of_authors <- str_extract(data_locations$institution, city_names_pattern)
# 
# # Add the extracted city names to our dataset:
# data_locations_with_city<- data_locations %>%
#   bind_cols(cities_of_authors) %>%
#   rename(cities_of_authors = 18)
# 
# # Make the city name column consistent with paper dataset and remove any duplicates to avoid crazy erros when joining (I checked this doesn't results in mismatches):
# world_cities_filtered <- as_tibble(world_cities_filtered) %>%
#   rename(cities_of_authors = 1) %>%
#   distinct(cities_of_authors, .keep_all = TRUE)
# 
# #Join the world.cities dataset with our paper data so we have latitude and longer to for each city:
# data_locations_with_full_geo_location<- left_join(data_locations_with_city,
#            world_cities_filtered
#            )
# 
# # Now, break this to a CSV file because this took forever to extract the data we to want to have to do this every time I render this page!!
# write.csv(data_locations_with_full_geo_location, "posts/2023-history-of-behavioural-addictions-PubMed-part2/data_locations_with_full_geo_location.csv")



# Okay, now actually load pre-created data from Github:
url_geo_loc_data_link <- "https://raw.githubusercontent.com/rheirene/Quarto_Website/master/posts/2023-history-of-behavioural-addictions-PubMed-part2/data_locations_with_full_geo_location.csv"

data_locations_with_full_geo_location <- read_csv(url_geo_loc_data_link) %>%
  as_tibble()

```

```{r}
# Load world map data but filtered Antarctica as we don't have any values/papers for this region and it just takes up space on the map:
world_map <- map_data("world") %>% filter(region != "Antarctica")

# Join the world map data with our paper data:
data_locations_with_full_geo_location_WORLD<- left_join(data_locations_with_full_geo_location, world_map) %>%
  # Tidy the behavioural addiction labels:
    mutate(Label = str_replace_all(Label, "_", " ") %>%
                 str_to_title())

# Wrangle data for plot:
data_locations_with_full_geo_location_WORLD_city_aggregate<- data_locations_with_full_geo_location_WORLD %>%  
  # Calculate the unique number of papers that can be linked to each city:
   group_by(cities_of_authors) %>%
  summarise(
    num_papers = n_distinct(PMID),
    lat = first(lat),   # Retain the first value of latitude for the city
    long = first(long),  # Retain the first value of longitude for the city
    group = first(group) # Retain the group value from the world map dataset  
  ) %>%
  filter(!is.na(cities_of_authors)) %>% # Remove rows where we don't have a city
    arrange(desc(num_papers))   # Just for easier viewing

   
all_publications_map <- ggplot(world_map, aes(x = long, y = lat, group = group)) +
  geom_polygon(fill="#289998", colour = "#289998") +
   geom_point(data = data_locations_with_full_geo_location_WORLD_city_aggregate, 
              aes(x = long, y = lat, 
              size = sqrt(num_papers),
              text = paste("City:", cities_of_authors, "\nNo. of publications:", num_papers)),  
              colour = "#50B5C8",
              fill = "#F5F7F0",
              alpha = 1,
              shape = 21) +  # Use shape 21 for filled circles
  scale_size_continuous(guide="none") +
  scale_x_continuous(expand = c(.001, .001)) +
  # scale_y_continuous(expand = c(.001, .001)) +
  plot_theme +
  theme(
    axis.text = element_blank(),
    axis.title = element_blank()) +
  labs(
    title = "Institutions of authors publishing behavioural addictions research",
    subtitle = "Each circle represents a city and its size corresponds to the number of papers can be linked to it",
    caption = "Rob Heirene (@rheirene)"
  )

# Save this now as a basic plot:
ggsave("all_publications_map.svg", 
       plot = all_publications_map, 
       width = 9, 
       height = 6,
       dpi = 600)


# We're going to turn this into an interactive plot now using ggplotly, but this removes some of our existing theme settings, especially the fonts. The standard way of changing the font in ggplotly doesn't seem to work for me, and it seems like other people having the same issue. I found this workaround online (https://github.com/plotly/plotly.R/issues/2117) which I now use below to load and use the correct font once this becomes a ggplotly:

# Get the URL for the "Reem Kufi" font from Google Fonts:
reem_kufi_file <- showtextdb::google_fonts("Reem Kufi")$regular_url 

# Create custom CSS:
reem_kufi_css <- paste0(
  "<style type = 'text/css'>",
    "@font-face { ",
      "font-family: 'Reem Kufi'; ", 
      "src: url('", reem_kufi_file, "'); ",
    "}",
  "</style>"
)

# Convert static plot to ggplotly format and adjust theme settings where required: 
all_publications_map_ggplotly <- ggplotly(all_publications_map,
                                              tooltip = 'text') %>% 
  hide_legend() %>%
  plotly::layout(margin = list(t = 90), # Increase top margin
                 font = list(family = "Reem Kufi"),
                 title = list(x = 0, y = 0.945),
    hoverlabel = list(font = list(family = "Reem Kufi")
                 
))

# Add the CSS as a dependency for the plotly plot:
all_publications_map_ggplotly$dependencies <- c(
  all_publications_map_ggplotly$dependencies,
  list(
    htmltools::htmlDependency(
      name = "reem-kufi-font", 
      version = "0",  
      src = "",
      head = reem_kufi_css)))

# Display plot:
saveWidget(all_publications_map_ggplotly, 'all_pubs_map.html')
```

```{=html}
<iframe src="all_pubs_map.html" class="fade-inhtml" width="100%" height="600" style="border:none;"></iframe>
```
::: {.callout-tip appearance="minimal" icon="false"}
## Spot an error in the map?

Trying to match city names to author institutions listed in the data has taught me that there are a lot of duplicated/triplicated/quadruplicated (not sure if that's even a word) city names throughout the world. This sometimes means that the author institutions were linked to the wrong city when I first at this. I've tried to fix all of these errors (some manually, some using some quick workarounds), but if you think you spot an error like this then please do let me know!
:::

Well, that looks nice. I used the `ggplotly` package to make the map interactive so you can zoom into any area and hover over the circles to see which city is represented and how many papers can be linked to it.

```{r}
# Wrangle data for plot:
data_locations_with_full_geo_location_WORLD_city_label_aggregate<- data_locations_with_full_geo_location_WORLD %>%
 left_join(world_map) %>%
   # Calculate the unique number of papers PER ADDICTION that can be linked to each city:
   group_by(cities_of_authors,Label) %>%
  summarise(
    num_papers = n_distinct(PMID),
    lat = first(lat),   # Retain the first value of latitude for the city
    long = first(long),  # Retain the first value of longitude for the city
    group = first(group) # Retain the group value from the world map dataset  
  ) %>%
  filter(!is.na(cities_of_authors)) %>% # Remove rows where we can get the city
    arrange(desc(num_papers)) %>%  # Just for easier viewing
   # Tidy the behavioural addiction labels:
    mutate(Label = str_replace_all(Label, "_", " ") %>%
                 str_to_title()) %>% 
    mutate(labels_filter = case_when(Label == "Gambling" ~ "Gambling addiction",
                                       Label == "Gaming" ~ "Gaming addiction",
                                       Label == "Smart Phone" ~ "Smart Phone addiction",
                                       Label == "Exercise" ~ "Exercise addiction",
                                       Label == "Social Media" ~ "Social Media addiction",
                                       Label == "Tanning" |
                                       Label == "Selfie" |
                                       Label == "Crime" |
                                       Label == "Dance" |
                                       Label == "Joyriding" |
                                       Label == "Polysurgical" |
                                       Label == "Death" |
                                         Label == "Near Death" |
                                         Label == "Fortune Telling" |
                                         Label == "Love"
                                         ~ "The more obscure \"addiction\"",
                                       TRUE ~ "REMOVE")) %>% 
  filter(labels_filter != "REMOVE")  %>% 
  mutate(labels_filter = as.factor(labels_filter)) %>% 
  ungroup()


filtered_publications_map<- ggplot(world_map, aes(x = long, y = lat, group = group)) +
  geom_polygon(fill = "#289998", colour = "#289998") +
  geom_point(data = data_locations_with_full_geo_location_WORLD_city_label_aggregate, 
             aes(x = long, y = lat, 
                 size = sqrt(num_papers),
                  colour = labels_filter,
                 fill = labels_filter),  
             alpha = 1,
             shape = 21) +
  # Removal guides
  scale_size_continuous(guide = "none") +
  guides(fill = "none", color = "none") +
  scale_x_continuous(expand = c(0.001, 0.001)) +
  scale_y_continuous(expand = c(0.001, 0.001)) +
  plot_theme +
  theme(
    axis.text = element_blank(),
    axis.title = element_blank()) +
  labs(
    title = "{closest_state} papers",
    caption = "Rob Heirene (@rheirene)"
  ) +
  transition_states( ### NOT FILTER!
     labels_filter,
     transition_length = 3, 
     state_length = 4,
     wrap = FALSE
   ) 
  # enter_fly(y_loc = 0) + # entering data: fly in vertically from bottom
  # exit_fly(y_loc = 100) + # exiting data: fly out vertically to top...
  # exit_fade() # ...while color is fading


# Animate plot in GIF format:
animate(filtered_publications_map,
        fps = 20,
        end_pause = 70,
        duration = 25,
        width = 750, height = 550,
        type = "cairo")

```

```{r include=FALSE}
# Save GIF:
anim_save("filtered_publications_map.gif", animation = filtered_publications_map)
```

### Where are these published?

::: {.callout-tip collapse="true" appearance="minimal" icon="false"}
## Expand for session information

```{r}
#| code-fold: true
#| code-summary: "Code: Get session info"

session_info(pkgs = "attached")
```
:::

------------------------------------------------------------------------

```{=html}
<script src="https://giscus.app/client.js"
        data-repo="rheirene/Quarto_Website"
        data-repo-id="R_kgDOJ0d4fA"
        data-category-id="DIC_kwDOJ0d4fM4CXv7I"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="dark_dimmed"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script
```
If
