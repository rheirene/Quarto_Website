{
  "hash": "bf01b77cd56564d55534ce46eae6430a",
  "result": {
    "markdown": "---\ntitle: \"The history of behavioural addictions research (according to PubMed): Part 2\"\ndescription: \"\"\nauthor:\n  - name: Rob Heirene\ncategories: [Gambling] \ndate: 2023-08-25\ndraft: true \n---\n\n\n### Data & code set-up\n\nJust like in Part 1, I'll first load all the required packages and fonts for figures.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code: set-up\"}\n# Install and load the groundhog package to ensure consistency of the package versions used here:\n# install.packages(\"groundhog\") # Install\n\nlibrary(groundhog) # Load\n\n# List desired packages:\npackages <- c('readr', # Load dataset from GitHib\n              'tidyverse', # Clean, organise, and visualise data\n              'gt', #  table data\n              'gtExtras', # Add colours to gt tables\n              'plotly', # Add interactive elements to figures\n              'gganimate', # Make animated plots\n              'transformr', # Needed for certain animations (dumbell lines)\n              'png',# Helps render gganimate plots\n              'gifski', # Helps render gganimate plots\n              'rmarkdown', # Helps render gganimate plots\n              'av', # render gganimate plots as videos\n              'Cairo', # Anti-aliasing for the line plots (smoothing output)\n              'ggtext', # make fancy labels in plots\n              'sysfonts', # Special fonts for figures\n              'showtext', # Special fonts for figures\n              'htmlwidgets', # Make plotly plots HTML format for rendering in Quarto\n              'scico', # Colour palette\n              'maps', # Get map/geographic data for author locations\n              'stringr', # extract city names\n              'sessioninfo') # Detailed session info for reproducibility \n\n# Load desired package with versions specific to project start date:\ngroundhog.library(packages, \"2023-08-01\") \n\n# Load new font for figures/graphs\nfont_add_google(\"Poppins\", \"Poppins\")\nfont_add_google(\"Reem Kufi\", \"Reem Kufi\")\nshowtext_auto(enable = TRUE) \n\n\nplot_theme <-  theme(plot.background = element_rect(fill = \"#002B36\",  color = NA), # ADDING THIS NA REMOVES BORDER AROUND PLOT ON WEBSITE\n     panel.background = element_rect(fill = \"#002B36\"),\n     text = element_text(family = \"Reem Kufi\", color = \"#F5F7F0\"),\n     axis.text = element_text(color = \"#F5F7F0\", size = 13),\n     panel.grid = element_blank(),\n     plot.title = element_text(color = \"#F5F7F0\", size = 16),\n     plot.subtitle = element_text(color = \"#50B5C8\"),\n     plot.caption = element_text(color = \"#50B5C8\"))\n```\n:::\n\n\nNow I'll load in the dataset and do a little cleaning. Again,, I'm going to remove all publications from 2023 so that we only have data for complete years (see comments in the code chunk below for any other exclusions).\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code: load dataset\"}\nurl_behav_addic_data_link <- \"https://raw.githubusercontent.com/rheirene/pub-med-scape-behav-addictions/main/Data%20extraction/combined_results_clean.csv\"\n\nraw_data <- read_csv(url_behav_addic_data_link) %>%\n  as_tibble()\n\nstr(raw_data)\n# View(data)\n\n# Despite my best efforts with manual searching, my explorations of this dataset in R revealed that there are a few erratums/corrigendums and one notice of retraction included in the data. Let's remove these before moving forward:\nfiltered_data <- raw_data %>%\n  filter(str_detect(Publication_Type, \"Erratum\") | \n         str_detect(Publication_Type, \"corrigendum\") | \n         str_detect(Publication_Type, \"Retraction\")) %>% \n  distinct(PMID, .keep_all = TRUE)\n\n# Let's now remove these pubs and any from 2023 so we have data for all \"full\" years:\ndata <- raw_data %>% \n  anti_join(filtered_data) %>%\n    filter(Year != \"2023\") \n\n# View(data)\n```\n:::\n\n\n### Where do these papers come from?\n\nI want to to visualise this information, but it's going to be quite tricky as each paper has a variable number of authors and therefore institution addresses that are all listed in a single string within one column in the dataset. I'll have to separate out each author institution and then find a way to extract only the relevant information to be able to geo-locate them.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ***********************The below code is almost all commented out on purpose as the process of extracting and matching city names from the author address column is so computationally taxing that it takes a long time to process.  I've left the code here so that anyone can see how I did it, but I saved the results as a .csv file and now load the data like that***********************\n# as_tibble(data$Author_Address) # Take a look at how the author addresses are structured\n\n# Okay, so we're going to need to create an ID variable for each paper (this makes the string split and a nest below work bettter than relying on titles), Then split the author_address strings into separate addresses, then unnest these into new rows. \n\n# The un-nest doesn't seem to work well when we retain all of the columns in the dataset, so I do it with only id and institution (address) in the data, then join all of the rest of the data set to the unnested rows after this. For this to work, we need to create a dataset that has the ID variable in before splitting the string and and unnesting. Let's do that:\n#  data_id <-data %>%\n#   rowid_to_column(var = \"id\")\n# \n# # Now split the address  string and then  unnest it into multiple rows, and finally re-join with the main dataset\n# data_locations <- data_id %>%\n#   mutate(institution = str_split(Author_Address, \";\")) %>%\n#   select(id, institution) %>%\n#   unnest(institution) %>%\n#   full_join(data_id, by = \"id\") %>%\n#   # Whilst we're doing this, we'll also create a counter/number for each institution per paper\n#   group_by(id) %>%\n#   mutate(author_num = row_number()) %>%\n#   ungroup()\n#   # We can also pivot to wide format if that makes sense at any point:\n#   # pivot_wider(names_from = author_num,\n#   #             values_from = institution,\n#   #             names_prefix = \"author_\",\n#   #             values_fill = NA_character_)\n# \n# # View(data_locations) # Looks good!\n# \n# # Fortunately, the \"maps\"  package contains a list of city names that we can use to match with our author institutions. Let's load the relevant data:\n# ## Loading country data from package maps\n# data(world.cities)\n#  \n# world_cities_filtered <-  world.cities %>%\n#   group_by(name) %>%\n#   filter(pop == max(pop)) %>% # Okay, so this is imperfect, but when a city name is duplicated, this will filter to select only the one with the highest population. This is based on the assumption that papers are likely to come from more populated cities (i.e. those with universities). This may seem crude, but it solved many, many issues in the map with, for example, putting \"Cambridge\" in the Caribbean as one of the most actively publishing cities on behavioural addictions in the world....\n#   ungroup() %>% \n#   filter(!name %in% c(\"China\", # There is a city in Mexico called China, and including this in the dataset needed to pick up any papers published in China and link them to this city!\n#                         \"India\", # Same sort of issue (City in Africa)\n#                         \"San\",  # Same sort of issue (City in Africa, again)\n#                         \"Institut\" # This appears to be act somewhere around Azerbaijan, but I think it's getting picked up as a city when in fact it just is a string in the author address referring to a university!\n#                        ))\n# \n# \n# # Extract just the city names so we can try and match author locations using these:\n# city_names_from_world <- world_cities_filtered$name\n# \n# # Create a pattern of city names for matching with word boundaries:\n# city_names_pattern <- paste(\"\\\\b(\", paste(city_names_from_world, collapse = \"|\"), \")\\\\b\", sep = \"\")\n# \n# # Extract city namesusing stringr (the str_exact function extracts the first complete match from a string; the arguments are the string and then the match you're looking for)\n# cities_of_authors <- str_extract(data_locations$institution, city_names_pattern)\n# \n# # Add the extracted city names to our dataset:\n# data_locations_with_city<- data_locations %>%\n#   bind_cols(cities_of_authors) %>%\n#   rename(cities_of_authors = 18)\n# \n# # Make the city name column consistent with paper dataset and remove any duplicates to avoid crazy erros when joining (I checked this doesn't results in mismatches):\n# world_cities_filtered <- as_tibble(world_cities_filtered) %>%\n#   rename(cities_of_authors = 1) %>%\n#   distinct(cities_of_authors, .keep_all = TRUE)\n# \n# #Join the world.cities dataset with our paper data so we have latitude and longer to for each city:\n# data_locations_with_full_geo_location<- left_join(data_locations_with_city,\n#            world_cities_filtered\n#            )\n# \n# # Now, break this to a CSV file because this took forever to extract the data we to want to have to do this every time I render this page!!\n# write.csv(data_locations_with_full_geo_location, \"posts/2023-history-of-behavioural-addictions-PubMed-part2/data_locations_with_full_geo_location.csv\")\n\n\n\n# Okay, now actually load pre-created data from Github:\nurl_geo_loc_data_link <- \"https://raw.githubusercontent.com/rheirene/Quarto_Website/master/posts/2023-history-of-behavioural-addictions-PubMed-part2/data_locations_with_full_geo_location.csv\"\n\ndata_locations_with_full_geo_location <- read_csv(url_geo_loc_data_link) %>%\n  as_tibble()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nNew names:\nRows: 35824 Columns: 24\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(15): institution, Title, Month, DOI, Abstract, Full_Author_Name, Author... dbl\n(9): ...1, id, PMID, Year, author_num, pop, lat, long, capital\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -> `...1`\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load world map data but filtered Antarctica as we don't have any values/papers for this region and it just takes up space on the map:\nworld_map <- map_data(\"world\") %>% filter(region != \"Antarctica\")\n\n# Join the world map data with our paper data:\ndata_locations_with_full_geo_location_WORLD<- left_join(data_locations_with_full_geo_location, world_map) %>%\n  # Tidy the behavioural addiction labels:\n    mutate(Label = str_replace_all(Label, \"_\", \" \") %>%\n                 str_to_title())\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining with `by = join_by(lat, long)`\n```\n:::\n\n```{.r .cell-code}\n# Wrangle data for plot:\ndata_locations_with_full_geo_location_WORLD_city_aggregate<- data_locations_with_full_geo_location_WORLD %>%  \n  # Calculate the unique number of papers that can be linked to each city:\n   group_by(cities_of_authors) %>%\n  summarise(\n    num_papers = n_distinct(PMID),\n    lat = first(lat),   # Retain the first value of latitude for the city\n    long = first(long),  # Retain the first value of longitude for the city\n    group = first(group) # Retain the group value from the world map dataset  \n  ) %>%\n  filter(!is.na(cities_of_authors)) %>% # Remove rows where we don't have a city\n    arrange(desc(num_papers))   # Just for easier viewing\n\n   \nall_publications_map <- ggplot(world_map, aes(x = long, y = lat, group = group)) +\n  geom_polygon(fill=\"#289998\", colour = \"#289998\") +\n   geom_point(data = data_locations_with_full_geo_location_WORLD_city_aggregate, \n              aes(x = long, y = lat, \n              size = sqrt(num_papers),\n              text = paste(\"City:\", cities_of_authors, \"\\nNo. of publications:\", num_papers)),  \n              colour = \"#50B5C8\",\n              fill = \"#F5F7F0\",\n              alpha = 1,\n              shape = 21) +  # Use shape 21 for filled circles\n  scale_size_continuous(guide=\"none\") +\n  scale_x_continuous(expand = c(.001, .001)) +\n  # scale_y_continuous(expand = c(.001, .001)) +\n  plot_theme +\n  theme(\n    axis.text = element_blank(),\n    axis.title = element_blank()) +\n  labs(\n    title = \"Institutions of authors publishing behavioural addictions research\",\n    subtitle = \"Each circle represents a city and its size corresponds to the number of papers can be linked to it\",\n    caption = \"Rob Heirene (@rheirene)\"\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in geom_point(data =\ndata_locations_with_full_geo_location_WORLD_city_aggregate, : Ignoring unknown\naesthetics: text\n```\n:::\n\n```{.r .cell-code}\n# Save this now as a basic plot:\nggsave(\"all_publications_map.svg\", \n       plot = all_publications_map, \n       width = 9, \n       height = 6,\n       dpi = 600)\n\n\n# We're going to turn this into an interactive plot now using ggplotly, but this removes some of our existing theme settings, especially the fonts. The standard way of changing the font in ggplotly doesn't seem to work for me, and it seems like other people having the same issue. I found this workaround online (https://github.com/plotly/plotly.R/issues/2117) which I now use below to load and use the correct font once this becomes a ggplotly:\n\n# Get the URL for the \"Reem Kufi\" font from Google Fonts:\nreem_kufi_file <- showtextdb::google_fonts(\"Reem Kufi\")$regular_url \n\n# Create custom CSS:\nreem_kufi_css <- paste0(\n  \"<style type = 'text/css'>\",\n    \"@font-face { \",\n      \"font-family: 'Reem Kufi'; \", \n      \"src: url('\", reem_kufi_file, \"'); \",\n    \"}\",\n  \"</style>\"\n)\n\n# Convert static plot to ggplotly format and adjust theme settings where required: \nall_publications_map_ggplotly <- ggplotly(all_publications_map,\n                                              tooltip = 'text') %>% \n  hide_legend() %>%\n  plotly::layout(margin = list(t = 90), # Increase top margin\n                 font = list(family = \"Reem Kufi\"),\n                 title = list(x = 0, y = 0.945),\n    hoverlabel = list(font = list(family = \"Reem Kufi\")\n                 \n))\n\n# Add the CSS as a dependency for the plotly plot:\nall_publications_map_ggplotly$dependencies <- c(\n  all_publications_map_ggplotly$dependencies,\n  list(\n    htmltools::htmlDependency(\n      name = \"reem-kufi-font\", \n      version = \"0\",  \n      src = \"\",\n      head = reem_kufi_css)))\n\n# Display plot:\nsaveWidget(all_publications_map_ggplotly, 'all_pubs_map.html')\n```\n:::\n\n```{=html}\n<iframe src=\"all_pubs_map.html\" class=\"fade-inhtml\" width=\"100%\" height=\"600\" style=\"border:none;\"></iframe>\n```\n\n::: {.callout-tip appearance=\"minimal\" icon=\"false\"}\n## Spot an error in the map?\n\nTrying to match city names to author institutions listed in the data has taught me that there are a lot of duplicated/triplicated/quadruplicated (not sure if that's even a word) city names throughout the world. This sometimes means that the author institutions were linked to the wrong city when I first at this. I've tried to fix all of these errors (some manually, some using some quick workarounds), but if you think you spot an error like this then please do let me know!\n:::\n\nWell, that looks nice. I used the `ggplotly` package to make the map interactive so you can zoom into any area and hover over the circles to see which city is represented and how many papers can be linked to it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Wrangle data for plot:\ndata_locations_with_full_geo_location_WORLD_city_label_aggregate<- data_locations_with_full_geo_location_WORLD %>%\n left_join(world_map) %>%\n   # Calculate the unique number of papers PER ADDICTION that can be linked to each city:\n   group_by(cities_of_authors,Label) %>%\n  summarise(\n    num_papers = n_distinct(PMID),\n    lat = first(lat),   # Retain the first value of latitude for the city\n    long = first(long),  # Retain the first value of longitude for the city\n    group = first(group) # Retain the group value from the world map dataset  \n  ) %>%\n  filter(!is.na(cities_of_authors)) %>% # Remove rows where we can get the city\n    arrange(desc(num_papers)) %>%  # Just for easier viewing\n   # Tidy the behavioural addiction labels:\n    mutate(Label = str_replace_all(Label, \"_\", \" \") %>%\n                 str_to_title()) %>% \n    mutate(labels_filter = case_when(Label == \"Gambling\" ~ \"Gambling addiction\",\n                                       Label == \"Gaming\" ~ \"Gaming addiction\",\n                                       Label == \"Smart Phone\" ~ \"Smart Phone addiction\",\n                                       Label == \"Exercise\" ~ \"Exercise addiction\",\n                                       Label == \"Social Media\" ~ \"Social Media addiction\",\n                                       Label == \"Tanning\" |\n                                       Label == \"Selfie\" |\n                                       Label == \"Crime\" |\n                                       Label == \"Dance\" |\n                                       Label == \"Joyriding\" |\n                                       Label == \"Polysurgical\" |\n                                       Label == \"Death\" |\n                                         Label == \"Near Death\" |\n                                         Label == \"Fortune Telling\" |\n                                         Label == \"Love\"\n                                         ~ \"The more obscure \\\"addiction\\\"\",\n                                       TRUE ~ \"REMOVE\")) %>% \n  filter(labels_filter != \"REMOVE\")  %>% \n  mutate(labels_filter = as.factor(labels_filter)) %>% \n  ungroup()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining with `by = join_by(lat, long, group, order, region, subregion)`\n`summarise()` has grouped output by 'cities_of_authors'. You can override using\nthe `.groups` argument.\n```\n:::\n\n```{.r .cell-code}\nfiltered_publications_map<- ggplot(world_map, aes(x = long, y = lat, group = group)) +\n  geom_polygon(fill = \"#289998\", colour = \"#289998\") +\n  geom_point(data = data_locations_with_full_geo_location_WORLD_city_label_aggregate, \n             aes(x = long, y = lat, \n                 size = sqrt(num_papers),\n                  colour = labels_filter,\n                 fill = labels_filter),  \n             alpha = 1,\n             shape = 21) +\n  # Removal guides\n  scale_size_continuous(guide = \"none\") +\n  guides(fill = \"none\", color = \"none\") +\n  scale_x_continuous(expand = c(0.001, 0.001)) +\n  scale_y_continuous(expand = c(0.001, 0.001)) +\n  plot_theme +\n  theme(\n    axis.text = element_blank(),\n    axis.title = element_blank()) +\n  labs(\n    title = \"{closest_state} papers\",\n    caption = \"Rob Heirene (@rheirene)\"\n  ) +\n  transition_states( ### NOT FILTER!\n     labels_filter,\n     transition_length = 3, \n     state_length = 4,\n     wrap = FALSE\n   ) \n  # enter_fly(y_loc = 0) + # entering data: fly in vertically from bottom\n  # exit_fly(y_loc = 100) + # exiting data: fly out vertically to top...\n  # exit_fade() # ...while color is fading\n\n\n# Animate plot in GIF format:\nanimate(filtered_publications_map,\n        fps = 20,\n        end_pause = 70,\n        duration = 25,\n        width = 750, height = 550,\n        type = \"cairo\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.gif)\n:::\n:::\n\n\n\n\n### Where are these published?\n\n::: {.callout-tip collapse=\"true\" appearance=\"minimal\" icon=\"false\"}\n## Expand for session information\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code: Get session info\"}\nsession_info(pkgs = \"attached\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16 ucrt)\n os       Windows 10 x64 (build 19045)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United Kingdom.utf8\n ctype    English_United Kingdom.utf8\n tz       Europe/London\n date     2023-08-30\n pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version  date (UTC) lib source\n av          * 0.8.3    2023-02-05 [1] CRAN (R 4.3.0)\n Cairo       * 1.6-0    2022-07-05 [1] CRAN (R 4.3.0)\n dplyr       * 1.1.2    2023-04-20 [1] CRAN (R 4.3.1)\n forcats     * 1.0.0    2023-01-29 [1] CRAN (R 4.3.1)\n gganimate   * 1.0.8    2022-09-08 [1] CRAN (R 4.3.1)\n ggplot2     * 3.4.2    2023-04-03 [1] CRAN (R 4.3.1)\n ggtext      * 0.1.2    2022-09-16 [1] CRAN (R 4.3.1)\n gifski      * 1.12.0-1 2023-06-08 [1] CRAN (R 4.3.0)\n groundhog   * 3.1.0    2023-05-05 [1] CRAN (R 4.3.1)\n gt          * 0.9.0    2023-03-31 [1] CRAN (R 4.3.1)\n gtExtras    * 0.4.5    2022-11-28 [1] CRAN (R 4.3.1)\n htmlwidgets * 1.6.2    2023-03-17 [1] CRAN (R 4.3.1)\n lubridate   * 1.9.2    2023-02-10 [1] CRAN (R 4.3.1)\n maps        * 3.4.1    2022-10-30 [1] CRAN (R 4.3.1)\n plotly      * 4.10.2   2023-06-03 [1] CRAN (R 4.3.1)\n png         * 0.1-8    2022-11-29 [1] CRAN (R 4.3.0)\n purrr       * 1.0.1    2023-01-10 [1] CRAN (R 4.3.1)\n readr       * 2.1.4    2023-02-10 [1] CRAN (R 4.3.1)\n rmarkdown   * 2.23     2023-07-01 [1] CRAN (R 4.3.1)\n scico       * 1.4.0    2023-05-30 [1] CRAN (R 4.3.0)\n sessioninfo * 1.2.2    2021-12-06 [1] CRAN (R 4.3.1)\n showtext    * 0.9-6    2023-05-03 [1] CRAN (R 4.3.1)\n showtextdb  * 3.0      2020-06-04 [1] CRAN (R 4.3.1)\n stringr     * 1.5.0    2022-12-02 [1] CRAN (R 4.3.1)\n sysfonts    * 0.8.8    2022-03-13 [1] CRAN (R 4.3.1)\n tibble      * 3.2.1    2023-03-20 [1] CRAN (R 4.3.1)\n tidyr       * 1.3.0    2023-01-24 [1] CRAN (R 4.3.1)\n tidyverse   * 2.0.0    2023-02-22 [1] CRAN (R 4.3.1)\n transformr  * 0.1.4    2022-08-18 [1] CRAN (R 4.3.1)\n\n [1] C:/Users/rheirene/AppData/Local/R/win-library/4.3\n [2] C:/Program Files/R/R-4.3.1/library\n\n──────────────────────────────────────────────────────────────────────────────\n```\n:::\n:::\n\n:::\n\n------------------------------------------------------------------------\n\n\n```{=html}\n<script src=\"https://giscus.app/client.js\"\n        data-repo=\"rheirene/Quarto_Website\"\n        data-repo-id=\"R_kgDOJ0d4fA\"\n        data-category-id=\"DIC_kwDOJ0d4fM4CXv7I\"\n        data-mapping=\"pathname\"\n        data-strict=\"0\"\n        data-reactions-enabled=\"1\"\n        data-emit-metadata=\"0\"\n        data-input-position=\"bottom\"\n        data-theme=\"dark_dimmed\"\n        data-lang=\"en\"\n        crossorigin=\"anonymous\"\n        async>\n</script\n```\n\nIf\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}