{
  "hash": "89648e971f0a5c9b76ef18259cb77955",
  "result": {
    "markdown": "---\ntitle: \"The history of behavioural addictions research (according to PubMed): Part 2\"\ndescription: \"\"\nauthor:\n  - name: Rob Heirene\ncategories: [Gambling] \ndate: 2023-08-25\ndraft: true \n---\n\n\n### Data & code set-up\n\nJust like in Part 1, I'll first load all the required packages and fonts for figures.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code: load packages\"}\n# Install and load the groundhog package to ensure consistency of the package versions used here:\n# install.packages(\"groundhog\") # Install\n\nlibrary(groundhog) # Load\n\n# List desired packages:\npackages <- c('readr', # Load dataset from GitHib\n              'tidyverse', # Clean, organise, and visualise data\n              'gt', #  table data\n              'gtExtras', # Add colours to gt tables\n              'plotly', # Add interactive elements to figures\n              'gganimate', # Make animated plots\n              'transformr', # Needed for certain animations (dumbell lines)\n              'png',# Helps render gganimate plots\n              'gifski', # Helps render gganimate plots\n              'rmarkdown', # Helps render gganimate plots\n              'av', # render gganimate plots as videos\n              'Cairo', # Anti-aliasing for the line plots (smoothing output)\n              'ggtext', # make fancy labels in plots\n              'sysfonts', # Special fonts for figures\n              'showtext', # Special fonts for figures\n              'htmlwidgets', # Make plotly plots HTML format for rendering in Quarto\n              'scico', # Colour palette\n              'maps', # Get map/geographic data for author locations\n              'purrr', # Help unnest city and author names across papers  equally\n              'stringr', # extract city names\n              'viridis', # More colour palettes\n              'sessioninfo') # Detailed session info for reproducibility \n\n# Load desired package with versions specific to project start date:\ngroundhog.library(packages, \"2023-08-01\") \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code: fonts & themes\"}\n# Load new font for figures/graphs\nfont_add_google(\"Poppins\", \"Poppins\")\nfont_add_google(\"Reem Kufi\", \"Reem Kufi\")\nshowtext_auto(enable = TRUE) \n\n\nplot_theme <-  theme(plot.background = element_rect(fill = \"#002B36\",  color = NA), # ADDING THIS NA REMOVES BORDER AROUND PLOT ON WEBSITE\n     panel.background = element_rect(fill = \"#002B36\"),\n     text = element_text(family = \"Reem Kufi\", color = \"#F5F7F0\"),\n     axis.text = element_text(color = \"#F5F7F0\", size = 13),\n     panel.grid = element_blank(),\n     plot.title = element_text(color = \"#F5F7F0\", size = 16),\n     plot.subtitle = element_text(color = \"#50B5C8\", size = 12),\n     plot.caption = element_text(color = \"#50B5C8\"))\n```\n:::\n\n\nNow I'll load in the dataset and do a little cleaning. Again,, I'm going to remove all publications from 2023 so that we only have data for complete years (see comments in the code chunk below for any other exclusions).\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code: load dataset\"}\nurl_behav_addic_data_link <- \"https://raw.githubusercontent.com/rheirene/pub-med-scape-behav-addictions/main/Data%20extraction/combined_results_clean.csv\"\n\nraw_data <- read_csv(url_behav_addic_data_link) %>%\n  as_tibble()\n\nstr(raw_data)\n# View(data)\n\n# Despite my best efforts with manual searching, my explorations of this dataset in R revealed that there are a few erratums/corrigendums and one notice of retraction included in the data. Let's remove these before moving forward:\nfiltered_data <- raw_data %>%\n  filter(str_detect(Publication_Type, \"Erratum\") | \n         str_detect(Publication_Type, \"corrigendum\") | \n         str_detect(Publication_Type, \"Retraction\")) %>% \n  distinct(PMID, .keep_all = TRUE)\n\n# Let's now remove these pubs and any from 2023 so we have data for all \"full\" years:\ndata <- raw_data %>% \n  anti_join(filtered_data) %>%\n    filter(Year != \"2023\") \n\n# View(data)\n```\n:::\n\n\n### Where do these papers come from?\n\nI want to to visualise this information, but it's going to be quite tricky as each paper has a variable number of authors and therefore institution addresses, all of which are listed in a single (often messy) string within one column in the dataset. I'll have to separate out each author institution and then find a way to extract only the relevant information to be able to geo-locate them.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code: Extract location data for each paper\"}\n# ***********************The below code is almost all commented out on purpose as the process of extracting and matching city names from the author address column is so computationally taxing that it takes a long time to process.  I've left the code here so that anyone can see how I did it, but I saved the results as a .csv file and now load the data like that***********************\n\n# # as_tibble(data$Author_Address) # Take a look at how the author addresses are structured\n# \n# # Okay, so we're going to need to create an ID variable for each paper (this makes the string split and a nest below work bettter than relying on titles), then split the author_address strings into separate addresses, then unnest these into new rows. \n# \n# # Whilst I'm splitting and unnesting the author address: I'm going to simultaneously do this for the authors name column.\n# \n# \n# # The un-nest doesn't seem to work well when we retain all of the columns in the dataset, so I do it with only id and institution (address) in the data, then join all of the rest of the data set to the unnested rows after this. For this to work, we need to create a dataset that has the ID variable in before splitting the string and and unnesting. Let's do that:\n#  data_id <-data %>%\n#   rowid_to_column(var = \"id\")\n# \n#  pad_vector <- function(vec, len) {\n#   length(vec) <- len\n#   return(vec)\n#  }\n# \n# # Now split the author and address strings and then  unnest it into multiple rows, and finally re-join with the main dataset\n# data_locations <- data_id %>%\n#     mutate(\n#      institution = str_split(Author_Address, \";\"), # Split author address column into separate strings For each address\n#     author_names = str_split(Full_Author_Name, \";\") # Split author name column into separate strings For each Name\n#   ) %>%\n#   # The below code matches the number of author institutions and author names where discrepancies exist, so the unnest further below works:\n#    mutate(\n#     max_length = pmax(map_int(institution, length), map_int(author_names, length)),\n#     institution = map2(institution, max_length, ~ pad_vector(., .y)),\n#     author_names = map2(author_names, max_length, ~ pad_vector(., .y))\n#   ) %>%\n#   select(id, institution, author_names) %>%\n#   unnest(c(institution, author_names)) %>%\n#   # Looking at the data at this point, there's a lot of white space around the institutions and author names. Let's remember that now:\n#   mutate(institution = str_trim(institution, side = \"both\"),\n#          author_names = str_trim(author_names, side = \"both\")) %>%\n#   full_join(data_id, by = \"id\") %>%\n#   # Whilst we're doing this, we'll also create a counter/number for each institution per paper\n#   group_by(id) %>%\n#   mutate(author_num = row_number()) %>%\n#   ungroup()\n#   # We can also pivot to wide format if that makes sense at any point:\n#   # pivot_wider(names_from = author_num,\n#   #             values_from = institution,\n#   #             names_prefix = \"author_\",\n#   #             values_fill = NA_character_)\n# \n# # View(data_locations) # Looks good!\n# \n# # Uncomment from below this line\n# \n# # Fortunately, the \"maps\"  package contains a list of city names that we can use to match with our author institutions. Let's load the relevant data:\n# ## Loading country data from package maps\n# data(world.cities)\n# \n#  # The way the matching process works below is by picking the first match in the string, so removing all of the cities below actually leads to an increase in proper matches as the wrong matches are skipped over:\n# world_cities_filtered <-  world.cities %>%\n#   filter(!name %in% c(\"China\", # There is a city in Mexico called China, and including this in the dataset needed to pick up any papers published in China and link them to this city!\n#                         \"India\", # Same sort of issue (City in Africa)\n#                         \"San\",  # Same sort of issue (City in Africa, again)\n#                         \"Institut\", # This appears to be a City somewhere around Azerbaijan, but I think it's getting picked up as a city when in fact it just is a string in the author address referring to a university!\n#                        \"Santa\", # This is picked up as a city In Peru, when in fact  with just part of the name of many different institutions\n#                       \"God\", # This is picked up as the name of a city in the Hungary, when in fact is just  part of the name of a hospital in Ireland\n#                       \"Normal\", # This is picked up as a city in the US  when in fact it's part of a university name in China\n#                       \"Bar\", # This is picked up as a city in Ukraine, when in fact it's just the name of the University in Israel\n#                       \"Victoria\", #  This leads to confusion between Victoria in Canada and the state in Australia. Easier just to remove rather than be inaccurate\n#                       \"Cardinal\",\n#                       \"Villanueva\", # This is actually a university in Spain, but it's picked up as a city in Honduras\n# \n#                       \"Beira\", # This is picked up as a city of Mozambique, but eventually the University Hospital name in Portugal\n#                       \"Cardinal\",\n#                       \"Young\", # This is picked up as a city in Uruguay, when is just part of the name of a young adult hospital in France\n#           \n#                       \"Cornwall\", # Get picked up as a city in Canada and not the area of the UK\n#                        \"George\", #  A street name in the US,  confused for the South African city\n#                        \"Imperial\", #  part of a UK university name but picked up as a city in Peru\n#                       \"Laval\", # Part of a university name in Canada but gets picked up as the city in France\n#                       \"Villa\", # Part of the name of a institution in Italy, but picked up as the Estonian city.\n#                       \"Aidu\", # Part of the name of a Japanese hospital, but picked up as a Estonian city\n#                      \"Carolina\", # Part of the US state name in the data, but picked up as a city in Peurto Rico\n#                        \"Carmel\", # Get confused with the US city, but always Israel in the dataset\n#                               \"U\",#  seems like shorthand for an address in France, but has picked up as a city in Micronesia and the French city is missed\n#                               \"Ramon\",  # Part of the name of a hospital in Spain, but picked up as the city in the Philippines\n#                              \"Fundacion\", #  Part of the name of a institution in Spain, but picked up as a city in Colombia\n#                              \"Trinidad\", # Part of a institution name in Argentina, were picked up as a city in Bolivia\n#                              \"Liege\",# Picked up as a city in Belgium, but it's actually part of a name of a place in France\n#                              \"East London\", # Refers to the UK University, picked up as a city in South Africa\n#                              \"Florida\", # University name picked up as a city in Cuba\n#                              \"Ita\", # Part of the name of a institution in Finland picked up as the city in Paraguay\n#                      \"Princeton\", #  University name confused for the city in Canada\n#                      \"Humboldt\", # Confused for the Canadian city, but actually a University in Germany\n#                       \"Alcala\", # Confused for the Colombian city, but actually part of a university name in Spain)\n#                             \"York\", # UK Canada confusion. Easier to remove\n#                      \"Union\", # Typically refers to the European Union, but confuse for the US city\n#                       \"La Rioja\", # Part of a Spanish University but computer the city in Argentina\n#                      \"Concord\", # Area in Australia that ends up being linked to a US city rather\n#                       \"Nanyang\", # Part of a Singapore university name that gets linked to a city in China\n#                       \"Patan\", # Part of a Napoleon University name that gets linked to a cityin India\n#                       \"Saint-Joseph\", # University name in Lebanon on that gets linked to a city in Reunion\n#                      \"Valencia\", # University in Spain that gets linked to Venezuela\n#                       \"Ingenio\", # Institution in Spain that gets linked to the Canary Islands\n#                      \"Lincoln\", # UK university name that gets linked to the US\n#                      \"Roma\", # Italian street name that gets linked to Australia\n#                      \"Leon\", # Institution name in France forgets links to Mexico\n#                      \"Pau\", # Part of a hospital name in Spain that Gets linked to France\n#                      \"Ilan\", # Part of a university name in Israel that gets linked to Taiwan\n#                      \"Street\", #  an obvious issue. Linked to the UK incorrectly\n#                      \"Alle\",  # Incorrectly linked to Switzerland when it's an address in Denmark'Hashtag\n#                      \"San Ignacio\", # Location improved its link to Bolivia\n#                      \"Carnot\", # Street in France that gets linked to central Africa\n#                       \"Mexico\", #  country gets incorrectly linked to the Philippines city\n#                       \"Mobile\", #  Institution in Canada that gets incorrectly linked to the US\n#                       \"Hebron\", # Location in Spain gets mixed up with Palestine\n#                      \"Liban\", #  hospital in France that gets linked to Czech Republic\n#                      \n#                      \"Bayonne\", # Addiction clinic in France linked to the US incorrectly\n#                      \"Apartado\", # Confusion between Spain and Colombia. Better to just remove\n#                      \"Rioja\", # Appears in a few different places and can be linked to Spain or Peru\n#                       \"Li\", #  part of a university name in China can be linked to Norway incorrectly\n#                      \"Al\", # Part of the name of places in Saudi Arabia and other countries that gets picked up as a city in Norway\n#                      \"San Agustin\", # Part of the name of a place in Peru that getting linked to Mexico\n#                      \"Asia\", # A city name in the Philippines that is obviously going to give problems\n#                      \"Jordan\", #  country name acts incorrectly linked to the Philippines city\n#                      \"Kota\", # Location in Malaysia that gets linked to India incorrectly\n#                      \"Ribera\", # Part of an institution name in Spain gets linked to Italy\n#                      \"Pilar\", # Name of a Institute in Croatia that gets linked to Brazil incorrectly\n#                       \"Greenwich\", # Causes various problems due to being linked to the US and UK\n#                       \"George Town\", #In Malaysia, baguettes linked to the Cayman Islands\n#                      \"Worth\", # Should the Fort Worth in America, baguettes link to Germany\n#                      \"Santa Lucia\", # Name of a institution in Italy that gets confused at the Canary Islands\n#                      \"Sainte Anne\", # Name of an institution in France that gets confused with the city in Canada\n#                      \"Douglas\", # Part of a university name inCanada gets confused with the Isle of Man\n#                      \"Arizona\", # State name gets confused for a city in Honduras\n#                      \"Potsdam\", # In New York gets confused with Germany\n#                      \"Kita\", # In the name of an Indonesian institution that gets confused with the city in Mali\n#                      \"Concordia\", # US university name gets confused with a town in Argentina\n#                      \"Bay\", # Monterey Bay gets confused with a town in the Philippines\n#                      \"Parana\", # Part of an institution name in Brazil gets linked to Argentina\n#                      \"Gazi\", # Incorrectly gets linked to Ken year when it should be part of a name of a university in Turkey\n#                      \"Wufeng\",# incorrectly linked to China when it should be in Taiwan\n#                      \"Loo\", # Getting correctly linked to Estonia when it's actually part of a institution name in Singapore\n#                      \"Police\", # Police college accidentally linked to city in Poland\n#                      \"Long\", # Thia city and in the name of yale uni address\n#                      \"David\", # City in Panama that causes obivous issues\n#                      \"Naval\", # City in the Philippines that causes obvious issues\n#                      \"Hall\", # City in the Philippines that causes obvious issues\n#                      \"Trinity\" # Irish uni name but gets mistaken to Jersey city\n#                      )) %>% \n#   # A combination of city names and country names can be used to keep the city where it seems like it can be saved:\n#     mutate(city_country = paste0(name,\", \", country.etc)) %>% \n#   filter(!city_country %in% c(\"Sussex, Canada\",# Only the UK one appears and this gets confused\n#                               \"Milton, Canada\", #  should be Milton Keynes in the UK\n#                               \"Bathurst, Canada\", # Location in Australia they get confused Canada\n#                               \"Milton, New Zealand\", #  should be Milton Keynes in the UK\n#                               \"Orleans, France\", #  incorrectly linked to France, not Canada\n#                               \"Bergen, Norway\", # teams to be linked consistently to the US,  but mistaken for Norway\n#                               \"Penrith, UK\", # Should be the Australian city near Sydney\n#                               \"Bedford, UK\", #  location in Australia gets Linked to the UK incorrectly\n#                               \"Bedford, USA\", #  location in Australia gets Linked to the US incorrectly\n#                               \"Salt, Spain\", # Should be Salt Lake City, US,  but gets picked up as the Spanish city\n#                               \"Lancaster, USA\", #  should be the UK\n#                               \"Ho, Ghana\", # Part of an address in Taiwan that's getting linked to Ghana\n#                               \"Laguna, USA\", # Should be the canary islands, surprisingly\n#                               \"Albert, France\",\n#                               \"Hong, Denmark\",# Leads to this being picked up and said of Hong Kong\n#                              \"Durham, USA\", # Should be the UK one\n#                              \"Brest, Belarus\", # Refers to France not Belarus\n#                              \"Warwick, USA\", #  Always seems to refer to the UK university, but the Use of the US city\n#                              \"Belmont, Canada\", #  Always seems to refer to the US,  but confused for the Canadian city\n#                              \"Beaufort, Malaysia\", # Should be the American city\n#                              \"Mackay, Australia\", #  should always be the location in Taiwan\n#                              \"Alicante, Philippines\", #  should always be in Spain\n#                              \"Malaya, Philippines\", # Should be in Malaysia\n#                            \"Claremont, Jamaica\", #  Australian location that gets sent to Jamaica, incorrectly\n#                            \n#                            \"Colombia, Cuba\", # Get confused with the country\n#                            \"Carlton, UK\", #Street name in Canada gets confused with the UK\n#                            \"Costa Rica, Mexico\", # Obviously get complete. The country\n#                            \"Notre Dame, Mauritius\", #  actually the Australian University!\n#                             \"Baja, Hungary\", # University namely Mexico\n#                            \"Palmerston, Australia\", #  links to the northern Australian city, rather than New Zealand\n#                            \"Waterloo, USA\" # Always the Canadian university\n#          )) %>% \n#    group_by(name) %>%\n#   filter(pop == max(pop)) %>% # Okay, so this is imperfect, but when a city name is duplicated that I haven't accounted for above, this will filter to select only the one with the highest population. This is based on the assumption that papers are likely to come from more populated cities (i.e. those with universities). This may seem crude, but it solved many, many issues in the map.\n#   ungroup() \n# \n# # Extract just the city names so we can try and match author locations using these:\n# city_names_from_world <- world_cities_filtered$name\n# \n# # Create a pattern of city names for matching with word boundaries:\n# city_names_pattern <- paste(\"\\\\b(\", paste(city_names_from_world, collapse = \"|\"), \")\\\\b\", sep = \"\")\n# \n# # Extract city names using stringr (the str_exact function extracts the first complete match from a string; the arguments are the string and then the match we're looking for)\n# cities_of_authors <- str_extract(data_locations$institution, city_names_pattern)\n# \n# \n# \n# # Before we join this to our dataset, let's now do the same thing for countries, using the world dataset. We could just link countries to the existing cities we identified, but this won't give us the richest overall data, as in some cases it might find a country name but no city name and vice versa.\n# \n# # Extract just the Country names so we can try and match author locations using these:\n# country_names <- world.cities$country.etc\n# \n# # Create a pattern of Country names for matching with word boundaries:\n# country_names_pattern <- paste(\"\\\\b(\", paste(country_names, collapse = \"|\"), \")\\\\b\", sep = \"\")\n# \n# # Extract country names using stringr (the str_exact function extracts the first complete match from a string; the arguments are the string and then the match we're looking for)\n# countries_of_authors <- str_extract(data_locations$institution, country_names_pattern)\n# \n# \n# \n# # Add the extracted city and country names to our dataset:\n# data_locations_with_city_country<- data_locations %>%\n#   bind_cols(cities_of_authors,\n#             countries_of_authors) %>%\n#   rename(cities_of_authors = 19,\n#          countries_of_authors = 20)\n# # Check everything looks okay:\n# #  select(author_names,\n# #         cities_of_authors,\n# #         countries_of_authors) %>%\n# #  print(n=150)\n# \n# # Make the city name column consistent with paper dataset:\n# world_cities_filtered <- as_tibble(world_cities_filtered) %>%\n#   rename(cities_of_authors = name)\n# \n# #Join the world.cities dataset with our paper data so we have latitude and longitude for each city:\n# data_locations_with_full_geo_location<-\n#   left_join(data_locations_with_city_country,\n#            world_cities_filtered,\n#            by = join_by(cities_of_authors) # This is done purposely, as I want to check whether the country names matched in text above match with the country names linked to the city names. Any mismatches tell us a lot about whether it got the right city are not and how I filtered out most of the problematic cities above!\n#            )\n# \n# # Now, Save this to a CSV file because this took forever to extract the data we don't want to have to do this every time I render this page!!\n# write.csv(data_locations_with_full_geo_location, \"posts/2023-history-of-behavioural-addictions-PubMed-part2/data_locations_with_full_geo_location.csv\")\n```\n:::\n\n\nPhew, that was intense. It was computationally demanding to link \\>40,000 author addresses to one out of every single city and country name worldwide so i've saved the dataset to Github and nowI'll load it in from there and do some cleaning.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# The code here is terrible. I'm sorry!\n# Okay, now actually load pre-created data from Github:\nurl_geo_loc_data_link <- \"https://raw.githubusercontent.com/rheirene/Quarto_Website/master/posts/2023-history-of-behavioural-addictions-PubMed-part2/data_locations_with_full_geo_location.csv\"\n\ndata_locations_with_full_geo_location <- read_csv(url_geo_loc_data_link) %>%\n  as_tibble() \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nNew names:\nRows: 42537 Columns: 27\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(18): institution, author_names, Title, Month, DOI, Abstract, Full_Autho... dbl\n(9): ...1, id, PMID, Year, author_num, pop, lat, long, capital\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -> `...1`\n```\n:::\n\n```{.r .cell-code}\n# Looking for mismatches betweencountries matched to cities I extracted and countries matched by text (as I do below) is how I spotted most of the problematic cities.\n\n# names(data_locations_with_full_geo_location)\n\n# Install proper  spelling of names in world cities dataset:\n\nworld.cities %>%\n  # filter(name == \"Turin\")\n  filter(country.etc == \"Canary Islands\")\n  \n# Let's explore this data to see any mismatches\n data_locations_with_full_geo_location %>% \nfilter(countries_of_authors != country.etc) %>% # Identify and isolate mismatches\n  select(institution,\n       author_names,\n         cities_of_authors,\n         countries_of_authors,\n       country.etc) %>%\n  print(n=350) # Started with several thousand\n \n# Looking at the results above, it's clear we need to prioritise matched country (i.e., the one we extracted from the text) over the linked one (linked to city name from database) as mistake are rate with the matched one as it's simple text extraction, whereas the link from city to country can be fallible as there are multiple cities with the same nameand it could have matched with the wrong city.\n\n# There are a few important caveats to the above that we will need to directly recode for accuracy:\n# Georgia USA. This gets extracted as the country, but it's obviously the state in the US! Also, the city in Georgia, Athens gets links to Greece incorrectly. \n# Mongolia.  Inner Mongolia normal University appears to be in China, but gets extracted as in Mongolia \n# Mexico. The University of New Mexico often gets linked to the country Mexico when it should be the US\n# Jersey. Gets linked to the small country south of the UK, instead of the University of New Jersey!\n \n# There are also a few key cities that are duplicated that I didn't remove in the filtering above that will need directly linking via code to the right city and country, including Cambridge, New York, London, Oxford, Liverpool, Bristol, Reading, Columbia, Northampton, Stirling, Aberdeen and Newport\n\n # Check allpotential problematic locations:\n data_locations_with_full_geo_location %>%\n   # filter(countries_of_authors  == \"Georgia\") %>% \n   # filter(countries_of_authors  == \"Mongolia\") %>% \n   # filter(countries_of_authors  == \"Mexico\") %>% \n  # filter(countries_of_authors  == \"Jersey\") %>% \n   # filter(countries_of_authors  == \"Jersey\") %>% \n   # filter(str_detect(institution, \"Columbia\")) %>% # Check anywhere for this one!\n  #  filter(str_detect(institution, \"Colombia\")) %>% # Check anywhere for this one!\n  # filter(str_detect(cities_of_authors, \"Marietta\")) %>% \n   # filter(str_detect(cities_of_authors, \"Liverpool\")) %>%\n   # filter(str_detect(cities_of_authors, \"Reading\")) %>%\n   # filter(str_detect(cities_of_authors, \"Northampton\")) %>%\n   # filter(str_detect(cities_of_authors, \"Stirling\")) %>%\n   # filter(str_detect(cities_of_authors, \"Cambridge\")) %>% \n # filter(str_detect(cities_of_authors, \"Aberdeen\")) %>%\n # filter(str_detect(cities_of_authors, \"Newport\")) %>%\n  # filter(str_detect(cities_of_authors, \"Oxford\")) %>%\n   filter(str_detect(institution, \"Kfar-Saba\")) %>%\n    select(institution,\n       author_names,\n         cities_of_authors,\n         countries_of_authors,\n       country.etc) %>%\n  print(n=350)\n \ndata_locations_with_full_geo_location_cleaned <- \n data_locations_with_full_geo_location %>%\n  mutate(cities_of_authors = case_when(\n    # Sort Georgia country/Athens city issues:\n    str_detect(institution, \"Georgia State\") ~ \"Atlanta\",\n    str_detect(institution, \"University of Georgia\") ~ \"Athens\",\n    str_detect(institution, \"Georgia College\") ~ \"Milledgeville\",\n    #  Mongolia city issues:\n    str_detect(institution, \"Inner Mongolia\") ~ \"Hothot\",\n    # Mexico City issues:\n    str_detect(institution, \"University of New Mexico\") ~ \"Albuquerque\",\n    # Jersey issues:\n    str_detect(institution, \"Rutgers\") &\n    !str_detect(institution, \"Camden\") &\n    !str_detect(institution, \"New York\") ~ \"New Brunswick\", # this isn't perfect, but cities are often missing for Rutgers\n    # London city issues\n     str_detect(institution, \"University College London\") ~ \"London\",\n     str_detect(institution, \"London, United Kingdom\") ~ \"London\",\n     str_detect(institution, \"Grovelands Priory Hospital\") ~ \"London\",\n   # Sort Columbia issues:\n    str_detect(institution, \"Vancouver\") ~ \"Vancouver\",\n    str_detect(institution, \"Columbia University\") ~ \"New York\",\n  # Sort Colombia issues:\n    str_detect(institution, \"Bogota\") ~ \"Bogota\",\n    str_detect(institution, \"Barranquilla\") ~ \"Barranquilla\",\n    str_detect(institution, \"Pasto\") ~ \"Pasto\",\n    str_detect(institution, \"Campus Robledo\") ~ \"Medellin\",\n   # Cambridge city issues:\n   str_detect(institution, \"Cambridge University, UCL and NHS National Centre for gaming Disorders\") ~ \"London\",\n   # UBC & Canada other:\n    str_detect(institution, \"University of British Columbia\") ~ \"Vancouver\",\n   str_detect(institution, \"Toronto, Canada\") ~ \"Toronto\",\n  str_detect(institution, \"Department of Education, Centre for Addiction and Mental Health\") ~ \"Toronto\",\n    str_detect(institution, \"Morton and Gloria Shulman Movement Disorders Clinic\") ~ \"Toronto\",\n          str_detect(institution, \"University of New Brunswick\") ~ \"Fredericton\",\n  str_detect(institution, \"Addiction & Mental Health Services-Kingston\") ~ \"Kingston\",\n  \n   # Aberdeen issues:\n   str_detect(institution, \"Hong Kong\") ~ \"Hong Kong\", # CHECK 312 and lower\n   # Newport issues:\n   str_detect(institution, \"Christopher Newport University\") ~ \"Newport News\",\n   # Cyprus:\n   str_detect(institution, \"Karsiyaka\") ~ \"Karsiyaka\",\n   # Palo Alto\n   str_detect(institution, \"Palo Alto\") ~ \"Palo Alto\",\n   # UWV:\n   str_detect(institution, \"Morgantown\") ~ \"Morgantown\",\n   # Marid:\n     str_detect(institution, \"Madrid, Spain\") ~ \"Madrid\",\n   # Paris:\n    str_detect(institution, \"Pole paris 12\") ~ \"Paris\",\n   str_detect(institution, \"Paris, France\") ~ \"Paris\",\n  str_detect(institution, \"Centre Pierre Nicole\") ~ \"Paris\",\n    str_detect(institution, \"centre hospitalier Sainte-Anne\") ~ \"Paris\",\n  \n  \n  # Providence:\n      str_detect(institution, \"Brown University\") ~ \"Providence\",\n  # North Kingston, RI, USA\n      str_detect(institution, \"North Kingston\") ~ \"Providence\",\n  # Essesx uni campus:\n  str_detect(institution, \"University of Essex, Colchester\") ~ \"Colchester\",\n  # Serbia uni & Belgrade:\n      str_detect(institution, \"Novi Pazar, Serbia\") ~ \"Novi Pazar\",\n    str_detect(institution, \"Belgrade, Serbia\") ~ \"Belgrade\",\n  # Milan:\n   str_detect(institution, \"Milan, Italy\") ~ \"Milan\",\n  # UMissouri:\n   str_detect(institution, \"University of Missouri\") ~ \"Columbia\",\n  # Rochester:\n   str_detect(institution, \"Rochester\") ~ \"Rochester\",\n  # Bedford, USA:\n  str_detect(institution, \"Edith Nourse Rogers Memorial Hospital\") ~ \"Bedford\",\n# Yale:\n  str_detect(institution, \"New Haven, CT\") ~ \"New Haven\",\n  str_detect(institution, \"Yale University\") ~ \"New Haven\",\n# Auckland & Newzeland:\n  str_detect(institution, \"Auckland\") ~ \"Auckland\",\n str_detect(institution, \"Palmerston North, Manawatu\") ~ \"Palmerston North\",\n# Lausanne:\n  str_detect(institution, \"Lausanne\") ~ \"Lausanne\",\n# Boston issues:\n  str_detect(institution, \"Boston, MA\") ~ \"Boston\",\n  str_detect(institution, \"Veteran's MH and Addiction Program, VA\") ~ \"Boston\",\n  str_detect(institution, \"Berenson-Allen Center for Noninvasive\") ~ \"Boston\",\n# New york:\n   str_detect(institution, \"Nassau Community College\") ~ \"New York\",\nstr_detect(institution, \"St Bonaventure University\") ~ \"New York\",\nstr_detect(institution, \"Elmhurst Hospital Center\") ~ \"New York\",\nstr_detect(institution, \"Cure Huntington's Disease Initiative\") ~ \"New York\",\n\n\n# Amityville\nstr_detect(institution, \"Amityville\") ~ \"Amityville\",\n# German city:\n   str_detect(institution, \"Villingen-Schwenningen\") ~ \"Villingen-Schwenningen\",\n  str_detect(institution, \"Hurth, Germany\") ~ \"Hurth\",\nstr_detect(institution, \"Martin-Luther-University\") ~ \"Halle\",\n# Iraninan city:\n  str_detect(institution, \"Isfahan, Iran\") ~ \"Isfahan\",\n# Israel:\nstr_detect(institution, \"Kfar-Saba\") ~ \"Tel Aviv\",\nstr_detect(institution, \"Leslie and Susan Gonda\") ~ \"Tel Aviv\",\t\n\n# South Korea:\nstr_detect(institution, \"Hanyang University\") ~ \"Seoul\",\nstr_detect(institution, \"Chungmugong Leadership Center\") ~ \"Changwon\",\nstr_detect(institution, \"Korea Institute on Behavioral Addictions\") ~ \"Seoul\",\nstr_detect(institution, \"hallym University\") ~ \"Anyang\",\n# Christiana Care Hospital:  \nstr_detect(institution, \"Christiana Care Hospital\") ~ \"Wilmington\", \n# Rush University Medical Center:\nstr_detect(institution, \"Rush University Medical Center\") ~ \"Chicago\",\n # Jordan city:\n str_detect(institution, \"Amman 19392, Jordan\") ~ \"Amman\", \n# Perth/Aus:\n str_detect(institution, \"Perth, Australia\") ~ \"Perth\", \n str_detect(institution, \"Adelaide\") ~ \"Adelaide\", \nstr_detect(institution, \" CQUniversity, 400 Kent St, Sydney\") ~ \"Sydney\", \n# Oxford Uni:\n str_detect(institution, \"Oxford, United Kingdom\") ~ \"Oxford\", \n# Salford Uni:\n str_detect(institution, \"Frederick Road Campus\") ~ \"Salford\", \n # Italian cities:\n str_detect(institution, \"Portici, Italy\") ~ \"Naples\", \nstr_detect(institution, \"Betania Evangelical Hospital\") ~ \"Naples\", \n str_detect(institution, \"Genoa, Italy\") ~ \"Genoa\", \n str_detect(institution, \"Lecco, Italy\") ~ \"Lecco\", \nstr_detect(institution, \"University of Genova\") ~ \"Genoa\", \nstr_detect(institution, \"Terni, Italy\") ~ \"Terni\", \nstr_detect(institution, \"Telese Terme\") ~ \"Telese\",\nstr_detect(institution, \"Urbino\") ~ \"Urbino\", \nstr_detect(institution, \"Universita Cattolica\") ~ \"Rome\", \nstr_detect(institution, \"University of Turin, Torino\") ~ \"Turin\", \nstr_detect(institution, \"Alma Mater Studiorum\") ~ \"Bologna\", \n\n\n  # barcelona & other spanish city issues:\n  str_detect(institution, \"Barcelona, Spain\") ~ \"Barcelona\",\n  str_detect(institution, \"Universitat Rovira i Virgili\") ~ \"Tarragona\",\n  str_detect(institution, \"Alicante, Spain\") ~ \"Alicante\",\n  str_detect(institution, \"Bormujos, Spain\") ~ \"Bormujos\",\nstr_detect(institution, \"Ciencies d'Alimentacio\") ~ \"Barcelona\",\nstr_detect(institution, \"Jimenez Diaz University Hospital\") ~ \"Mardrid\",\nstr_detect(institution, \"Centro Universitario Cardenal Cisneros\") ~ \"Madrid\", \nstr_detect(institution, \"Santiago de Compostela\") ~ \"Santiago de Compostela\", \nstr_detect(institution, \"Consumer and User Psychology Unit, Faculty of Psychology, University of Santiago\") ~ \"Santiago de Compostela\", \nstr_detect(institution, \"Department of Physiology, School of Medicine, University of Santiago de\") ~ \"Santiago de Compostela\", # Confirmed by searching original paper\nstr_detect(institution, \"Serra Hunter Programme\") ~ \"Barcelona\",\nstr_detect(institution, \"Hospital Universitario de Canarias\") ~ \"Santa Cruz de Tenerife\",\nstr_detect(institution, \"Universitat Pompeu Fabra\") ~ \"Barcelona\",\nstr_detect(institution, \"Universidad Loyola Andalucia\") ~ \"Cordoba\",\n\n\n\n\n# Nottingham:\n str_detect(institution, \"Newark Beacon Innovation Centre\") ~ \"Nottingham\",\n\n# Portugal:\nstr_detect(institution, \"Unity in Multidisciplinary Research on Biomedicine (UMIB)\") ~ \"Porto\",\nstr_detect(institution, \"IAJ (Gambling Support Institute)\") ~ \"Lisbon\",\n\n  # Porto Alegre issue:\n  str_detect(institution, \"Porto Alegre\") ~ \"Porto Alegre\",\n# Namur:\n str_detect(institution, \"Namur\") ~ \"Namur\", # Checked there's only one in the data\n# Sandy bay tas:\n str_detect(institution, \"Sandy Bay\") ~ \"Sandy Bay\",\n# Oviedo:\n str_detect(institution, \"Oviedo, Spain\") ~ \"Oviedo\",\n str_detect(institution, \"Virginia Tech, Blacksburg, VA\") ~ \"Roanoke\",\n# Dublin & ireland:\n str_detect(institution, \"Lucena Clinic Rathgar\") ~ \"Dublin\",\n str_detect(institution, \"Dublin, Ireland\") ~ \"Dublin\",\n # West Chester University:\n   str_detect(institution, \"West Chester University\") ~ \"Philadelphia\",\n  # Yunlin, Taiwan:\n  str_detect(institution, \"Yunlin, Taiwan\") ~ \"Douliu\",\n    # University of Montana:\n  str_detect(institution, \"University of Montana\") ~ \"Missoula\",\n      # San Juan:\n  str_detect(institution, \"University of Puerto Rico, San Juan\") ~ \"San Juan\",\n        # Moroccan city:\n    str_detect(institution, \"Fez, Morocco\") ~ \"Fez\",\n  # Seton Hall University & other newark issues:\n  str_detect(institution, \"Seton Hall University\") ~ \"Newark\",\n    str_detect(institution, \"Parsippany, NJ\") ~ \"Newark\",\n# Auburn University:\nstr_detect(institution, \"Auburn University\") ~ \"Montgomery\",\n#  University of Manitoba:\nstr_detect(institution, \"University of Manitoba\") ~ \"Winnipeg\",\n# University of South Dakota:\nstr_detect(institution, \"University of South Dakota\") ~ \"Vermillion\",\n# University of Antwerpen:\nstr_detect(institution, \"Institute Born-Bunge\") ~ \"Antwerp\",\n# University of Kansas:\nstr_detect(institution,  \"University of Kansas\") ~ \"Lawrence\",\n# Vanderbilt University:\nstr_detect(institution,  \"Vanderbilt\") ~ \"Nashville\",\n# Wayne state university:\nstr_detect(institution,  \"Wayne State University\") ~ \"Detroit\",\n# University  Michigan:\nstr_detect(institution,  \"University of Michigan\") ~ \"Ann Arbor\",\n# Carson College of Business:\nstr_detect(institution,  \"Carson College of Business\") ~ \"Pullman\",\n# Universidad de San Martin de Porres\nstr_detect(institution,  \"Universidad de San Martin de Porres\") ~ \"Lima\",\n# The Center for Internet and Technology Addiction\nstr_detect(institution,  \"The Center for Internet and Technology Addiction\") ~ \"Hartford\",\n  # Meridian Behavioral Health Services (I can't find a full address for this, despite searching the people manually, But it doesn't appear to be in the US):\n  str_detect(institution,  \"Meridian Behavioral Health Services\") ~ \"\",\n  # Brazilian hospital:\n     str_detect(institution, \"Belo Horizonte\") ~ \"Brazil\",\n  # Nashville:\n    str_detect(institution, \"Nashville, TN\") ~ \"Nashville\",\n    # Fort Lauderdale:\n    str_detect(institution, \"Fort Lauderdale\") ~ \"Fort Lauderdale\",\n      # Byblos Lebanon:\n    str_detect(institution, \"Byblos, Lebanon\") ~ \"Byblos\",\n # Belgium city:\n    str_detect(institution, \"Hasselt, Belgium\") ~ \"Hasselt\",\n# Poland:\n str_detect(institution, \"Uniwersytet Jagiellonski Collegium Medicum\") ~ \"Krakow\",\n # Peru:\n  str_detect(institution, \"Lima, Peru\") ~ \"Lima\",\n#Japan:\nstr_detect(institution, \"Konan Women's University\") ~ \"Kobe\",\nstr_detect(institution, \"Tokai Gakuen University\") ~ \"Tokai Gakuen University\",\nstr_detect(institution, \"University of Hokkaido\") ~ \"Hokkaido\",\nstr_detect(institution, \"Tama-ku, Kawasaki\") ~ \"Kawasaki\",\n\n\n\n# Virginia:\n str_detect(institution, \"Virginia Institute for Psychiatry and Behavioral Genetics\") ~ \"Richmond\",\n# Kuala Lumpur:\n str_detect(institution, \"Monash University Malaysia\") ~ \"Kuala Lumpur\",\n # Vietnam:\n str_detect(institution, \"Nguyen Tat Thanh University\") ~ \"Ho Chi Minh City\",\n   # Santo André:\n    str_detect(institution, \"Santo Andre, SP, Brazil\") ~ \"Santo Andre\",\n# author not institution:\nstr_detect(institution, \"Maya Sahu, RN, RM,\") ~ \"\",\n    TRUE ~ as.character(cities_of_authors)  # Default if none of the above matches\n\n\n\n # COUNTRY CHANGES:\n )) %>%\n  mutate(countries_of_authors = case_when(\n # Sort Georgia country issues:\n    str_detect(institution, \"Georgia State University\") ~ \"USA\",\n    str_detect(institution, \"University of Georgia\") ~ \"USA\",\n    str_detect(institution, \"Georgia College\") ~ \"USA\",\n    str_detect(cities_of_authors, \"Marietta\") ~ \"USA\",\n  # Mongolia country issues:\n    str_detect(institution, \"Inner Mongolia\") ~ \"China\",\n  # Mexico Country issues:\n    str_detect(institution, \"University of New Mexico\") ~ \"USA\",\n  # Germany issues:\n  str_detect(institution, \"Martin-Luther-University\") ~ \"Germany\",\n  # Jersey country issues:\n    str_detect(institution, \"New Jersey\") ~ \"USA\",\n  # Cambridge city issues:\n    str_detect(institution, \"Harvard\") ~ \"USA\",\n    str_detect(institution, \"University of Cambridge\") ~ \"UK\",\n    str_detect(institution, \"Cambridge Health Alliance\") ~ \"USA\",\n    str_detect(institution, \"Anglia Ruskin University\") ~ \"UK\",\n  # Boston issue:\n    str_detect(institution, \"Boston, MA\") ~ \"USA\",\n  # London city issues\n    str_detect(institution, \"London\")  &\n    !str_detect(institution, \"Ontario\") &\n    !str_detect(institution, \"Canada\") ~ \"UK\",\n     str_detect(institution, \"Ontario\") ~ \"Canada\",\n  # Sort Columbia issues:\n  \n    str_detect(institution, \"University of British Columbia\") ~ \"Canada\",\n    str_detect(institution, \"Columbia University\") ~ \"USA\",\n    str_detect(institution, \"United States of America\") ~ \"USA\",\n    str_detect(institution, \"Missouri\") ~ \"USA\",\n    str_detect(institution, \"New York\") ~ \"USA\",\n    str_detect(institution, \"British Columbia\") ~ \"Canada\",\n    str_detect(institution, \"Chilliwack\") ~ \"Canada\",\n  str_detect(institution, \"Centro Universitario Cardenal Cisneros\") ~ \"Spain\",\n  \n  # Australia:\n  str_detect(institution, \"Adelaide\") ~ \"Australia\",\n  str_detect(institution, \" CQUniversity, 400 Kent St, Sydney\") ~ \"Australia\",\n  str_detect(institution, \" CQUniversity\") ~ \"Australia\",\n   # Sort Colombia issues:\n     str_detect(institution, \"Barranquilla\") ~ \"Colombia\",\n  # Canada:\n    str_detect(institution, \"Department of Education, Centre for Addiction and Mental Health\") ~ \"Canada\",\n      str_detect(institution, \"Morton and Gloria Shulman Movement Disorders Clinic\") ~ \"Canada\",\n        str_detect(institution, \"University of New Brunswick\") ~ \"Canada\",\n    str_detect(institution, \"Addiction & Mental Health Services-Kingston\") ~ \"Canada\",\n  str_detect(institution, \"Toronto, ON\") ~ \"Canada\",\n  str_detect(institution, \"Social and Economic Impacts of Gambling in Massachusetts project,\") ~ \"USA\", # Was originally Canada\n  \n  \n  # South Korea:\nstr_detect(institution, \"Hanyang University\") ~ \"South Korea\",\nstr_detect(institution, \"Chungmugong Leadership Center\") ~ \"South Korea\",\nstr_detect(institution, \"Korea Institute on Behavioral Addictions\") ~ \"South Korea\",\nstr_detect(institution, \"hallym University\") ~ \"South Korea\",\n\n# Portugal:\nstr_detect(institution, \"Unity in Multidisciplinary Research on Biomedicine (UMIB)\") ~ \"Portugal\",\n  # China:\n    str_detect(institution, \"The Chinese University of Hong Kong\") ~ \"China\",\n\n# South Africa:\nstr_detect(institution, \"Christiana Care Hospital\") ~ \"USA\", \n\n# Newark Beacon Innovation Centre:\n str_detect(institution, \"Newark Beacon Innovation Centre\") ~ \"UK\",\n\n  # Sort Liverpool issues: \n  str_detect(institution, \"John Moores\") ~ \"UK\",\n  str_detect(institution, \"University of Liverpool\") ~ \"UK\",\n  str_detect(institution, \"LiMRIC\") ~ \"UK\",\n  str_detect(institution, \"Liverpool, England\") ~ \"UK\",\n   str_detect(institution, \"Liverpool John\") ~ \"UK\",\n  # Reading issues:\n  str_detect(institution, \"University of Reading\") ~ \"UK\",\n   str_detect(institution, \"USA\") ~ \"USA\",\n  str_detect(institution, \"United States\") ~ \"USA\",\n  # Northampton issues:\n  str_detect(institution, \"University of Northampton\") ~ \"UK\",\n  str_detect(institution, \"Gemini Research\") ~ \"USA\",\n  # Virginia:\n str_detect(institution, \"Virginia Institute for Psychiatry and Behavioral Genetics\") ~ \"USA\",\n # University of Alabama:\n   str_detect(institution, \"University of Alabama\") ~ \"USA\",\n# University  Michigan:\nstr_detect(institution,  \"University of Michigan\") ~ \"USAr\",\n # The Center for Internet and Technology Addiction\nstr_detect(institution,  \"The Center for Internet and Technology Addiction\") ~ \"USA\",\n#  University of Southern  California:\nstr_detect(institution,  \"University of Southern California\") ~ \"USA\",\n# Salford Uni:\n str_detect(institution, \"Frederick Road Campus\") ~ \"UK\", \n  # Sterling issues:\n   str_detect(institution, \"Australia\") ~ \"Australia\",\n   # Palo Alto\n   str_detect(institution, \"Palo Alto\") ~ \"USA\",\n   # Aberdeen issues:\n   str_detect(institution, \"Hong kong\") ~ \"China\",\n  # Newport issues:\n   str_detect(institution, \"Christopher Newport University\") ~ \"USA\",\n  # Oxford issues:\n   str_detect(institution, \"University of Oxford\") ~ \"UK\", \n  str_detect(institution, \"Oxford, UK\") ~ \"UK\", \n  str_detect(institution, \"Oxford Centre for\") ~ \"UK\",\n  # Weird barcelona uni & spain issue:\n  str_detect(institution, \"University of Barcelona, Barcelona\") ~ \"Spain\",\n  str_detect(institution, \"Jimenez Diaz University Hospital\") ~ \"Spain\",\nstr_detect(institution, \"Santiago de Compostela\") ~ \"Spain\", \nstr_detect(institution, \"Consumer and User Psychology Unit, Faculty of Psychology, University of Santiago\") ~ \"Spain\", \nstr_detect(institution, \"Serra Hunter Programme\") ~ \"Spain\",\nstr_detect(institution, \"Hospital Universitario de Canarias\") ~ \"Spain\",\nstr_detect(institution, \"Universitat Pompeu Fabra\") ~ \"Spain\",\nstr_detect(institution, \"Universidad Loyola Andalucia\") ~ \"Spain\",\n# Italy:\nstr_detect(institution, \"Urbino\") ~ \"Italy\", \nstr_detect(institution, \"Betania Evangelical Hospital\") ~ \"Italy\", \nstr_detect(institution, \"University of Turin, Torino\") ~ \"Italy\", \nstr_detect(institution, \"Alma Mater Studiorum\") ~ \"Italy\", \n\n# Brazil:\nstr_detect(institution, \"Porto Alegre\") ~ \"Brazil\", \n\n\n  # Lausanne issues:\n str_detect(institution, \"Lausanne\") ~ \"Switzerland\",\n # West Chester University:\n   str_detect(institution, \"West Chester University\") ~ \"USA\",\n#Rede SARAH de Hospitais de Reabilitacao:\n   str_detect(institution, \"Rede SARAH de Hospitais de Reabilitacao\") ~ \"Brazil\",\n # Taiwan:\n    str_detect(institution, \"Taichung\") ~ \"Taiwan\",\n  # Vietnam:\n str_detect(institution, \"Nguyen Tat Thanh University\") ~ \"Vietnam\",\n # Spain:\n str_detect(institution, \"Ciencies d'Alimentacio\") ~ \"Spain\",\n# Rush University Medical Center:\nstr_detect(institution, \"Rush University Medical Center\") ~ \"USA\",\n# Belgium:\nstr_detect(institution, \"Institute Born-Bunge\") ~ \"Belgium\",\n# Peru:\n# Universidad de San Martin de Porres\nstr_detect(institution,  \"Universidad de San Martin de Porres\") ~ \"Peru\",\n# Israel:\n str_detect(institution, \"Beit-Berl College\") ~ \"Israel\",\nstr_detect(institution, \"Leslie and Susan Gonda\") ~ \"Israel\",\n\n# author not institution:\nstr_detect(institution, \"Maya Sahu, RN, RM,\") ~ \"\",\n    TRUE ~ as.character(countries_of_authors)  # Default if none of the above matches\n  ))\n \n# Now look to see what's left and see if it matters now if we assume our created city and country names are correct and the link is an error:\n data_locations_with_full_geo_location_cleaned %>% \nfilter(countries_of_authors != country.etc) %>% # Identify and isolate mismatches\n  select(institution,\n       author_names,\n         cities_of_authors,\n         countries_of_authors,\n       country.etc) %>%\n  print(n = 340) # Okay, happy theses are good (after 10+ rounds of filtering)\n\n# Now look at papers where we couldn't country now look at countries where we couldn't match the country but linking the world.cities dataset connects a country with a city. Error check this:\n  data_locations_with_full_geo_location_cleaned %>% \n filter(is.na(countries_of_authors) & !is.na(country.etc)) %>% # Identify and isolate these instances\n    # Filter defined only unique instances to save time: \n     distinct(cities_of_authors, .keep_all = TRUE)  %>% \n  select(institution,\n         cities_of_authors,\n         countries_of_authors,\n       country.etc) %>%\n    # View()\n  print(n = 340) \n\n  \n # data_locations_with_full_geo_location_cleaned %>% \n#    filter(str_detect(institution, \"arson College of Business\")\n #          ) %>% \n    # View()\n# Okay,  I've gone back to the above recording section to correct any country mismatches found in the above output:\n\n    \n# need to try and linkj long and lat to the combination of city and country where possible\n\n \n # MAKE GIT FILE SMALLER FOR STORAGE\n\n # Other problematic  Cities we need to directly code:\n # Athens\n # Nottingham\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code: Create interactive map of papers\"}\n# Load world map data but filter out Antarctica as we don't have any values/papers for this region and it just takes up space on the map:\nworld_map <- map_data(\"world\") %>% \n  filter(region != \"Antarctica\")\n\n# Join the world map data with our paper data:\ndata_locations_with_full_geo_location_WORLD<- left_join(data_locations_with_full_geo_location_cleaned, world_map) %>%\n  # Tidy the behavioural addiction labels:\n    mutate(Label = str_replace_all(Label, \"_\", \" \") %>%\n                 str_to_title())\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining with `by = join_by(lat, long)`\n```\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code: Create interactive map of papers\"}\n# Wrangle data for plot:\ndata_locations_with_full_geo_location_WORLD_city_aggregate<- data_locations_with_full_geo_location_WORLD %>%\n  # Calculate the unique number of papers that can be linked to each city:\n   group_by(cities_of_authors) %>%\n  summarise(\n    num_papers = n_distinct(PMID),\n    lat = first(lat),   # Retain the first value of latitude for the city\n    long = first(long),  # Retain the first value of longitude for the city\n    group = first(group) # Retain the group value from the world map dataset\n  ) %>%\n  filter(!is.na(cities_of_authors)) %>% # Remove rows where we don't have a city\n    arrange(desc(num_papers)) %>%    # Just for easier viewing\n  ungroup() #  Doing this usually prevents issues later on!\n\n# It takes quite a long time to produce the interactive plot, so instead of waiting each time I run this script I'm just loading in the version I created earlier. Comment out all of the code below to create the plot.\n\n\n# # Create plot:\n# all_publications_map <- ggplot(world_map, aes(x = long, y = lat, group = group)) +\n#   geom_polygon(fill=\"#289998\", colour = \"#289998\") +\n#    geom_point(data = data_locations_with_full_geo_location_WORLD_city_aggregate,\n#               aes(x = long, y = lat,\n#             #  size = sqrt(num_papers),\n#               size = num_papers,\n#               text = paste(\"City:\", cities_of_authors, \"\\nNo. of publications:\", num_papers)),\n#               colour = \"#50B5C8\",\n#               fill = \"#F5F7F0\",\n#               alpha = 1,\n#               shape = 21) +  # Use shape 21 for filled circles\n#   scale_size_continuous(guide=\"none\") +\n#   scale_x_continuous(expand = c(.001, .001)) +\n#   # scale_y_continuous(expand = c(.001, .001)) +\n#   plot_theme +\n#   theme(\n#     axis.text = element_blank(),\n#     axis.title = element_blank()) +\n#   labs(\n#     title = \"Institutions of authors publishing behavioural addictions research\",\n#     subtitle = \"Each circle represents a city and its size corresponds to the number of papers can be linked to it\",\n#     caption = \"Rob Heirene (@rheirene)\"\n#   )\n# \n# # Save this now as a basic plot:\n# ggsave(\"posts/2023-history-of-behavioural-addictions-PubMed-part2/all_publications_map.svg\",\n#        plot = all_publications_map,\n#        width = 9,\n#        height = 6,\n#        dpi = 600)\n# \n# \n# # We're going to turn this into an interactive plot now using ggplotly, but this removes some of our existing theme settings, especially the fonts. The standard way of changing the font in ggplotly doesn't seem to work for me, and it seems like other people having the same issue. I found this workaround online (https://github.com/plotly/plotly.R/issues/2117) which I now use below to load and use the correct font once this becomes a ggplotly:\n# \n# # Get the URL for the \"Reem Kufi\" font from Google Fonts:\n# reem_kufi_file <- showtextdb::google_fonts(\"Reem Kufi\")$regular_url\n# \n# # Create custom CSS:\n# reem_kufi_css <- paste0(\n#   \"<style type = 'text/css'>\",\n#     \"@font-face { \",\n#       \"font-family: 'Reem Kufi'; \",\n#       \"src: url('\", reem_kufi_file, \"'); \",\n#     \"}\",\n#   \"</style>\"\n# )\n# \n# # Convert static plot to ggplotly format and adjust theme settings where required:\n#  all_publications_map_ggplotly <-ggplotly(all_publications_map,\n#                                               tooltip = 'text') %>%\n#   hide_legend() %>%\n#   plotly::layout(annotations =\n#  list(x = 1, y = -0.1, text = \"Rob Heirene (@rheirene)\",\n#       showarrow = F, xref='paper', yref='paper',\n#       xanchor='right', yanchor='auto', xshift=0, yshift=25,\n#       font=list(size=12, color=\"#50B5C8\")),\n#        margin = list(t = 70), # Increase top margin\n#                  font = list(family = \"Reem Kufi\"),\n#                  title = list(x = 0, y = 0.945),\n#     hoverlabel = list(font = list(family = \"Reem Kufi\")\n# ))\n# \n# # Add the CSS as a dependency for the plotly plot:\n# all_publications_map_ggplotly$dependencies <- c(\n#   all_publications_map_ggplotly$dependencies,\n#   list(\n#     htmltools::htmlDependency(\n#       name = \"reem-kufi-font\",\n#       version = \"0\",\n#       src = \"\",\n#       head = reem_kufi_css)))\n# \n# # Display plot:\n# saveWidget(all_publications_map_ggplotly, 'posts/2023-history-of-behavioural-addictions-PubMed-part2/all_pubs_map.html')\n```\n:::\n\n```{=html}\n<iframe src=\"all_pubs_map.html\" class=\"fade-inhtml\" width=\"100%\" height=\"600\" style=\"border:none;\"></iframe>\n```\n\nWell, that looks nice. I used the `ggplotly` package to make the map interactive so you can zoom into any area and hover over the circles to see which city is represented and how many papers can be linked to it.\n\n::: {.callout-tip appearance=\"minimal\" icon=\"false\"}\n## Spot an error in the map?\n\nTrying to match city names to author institutions listed in the data has taught me that there are a lot of duplicated/triplicated/quadruplicated (not sure if that's even a word) city names throughout the world. This sometimes means that the author institutions were linked to the wrong city when I first ran this. I've tried to fix all of these errors (some manually, some using some quick workarounds), but if you spot an error like this then please do let me know!\n:::\n\nL\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code: Create animated map of papers\"}\n# It takes quite a long time to produce the animated plot, so instead of waiting each time I run this script I'm just loading in the version I created earlier. Comment all of the code below to Cceate the plot.\n\n\n# # Wrangle data for plot:\n# data_locations_with_full_geo_location_WORLD_city_label_aggregate<- data_locations_with_full_geo_location_WORLD %>%\n#  left_join(world_map) %>%\n#    # Calculate the unique number of papers PER ADDICTION that can be linked to each city:\n#    group_by(cities_of_authors,Label) %>%\n#   summarise(\n#     num_papers = n_distinct(PMID),\n#     lat = first(lat),   # Retain the first value of latitude for the city\n#     long = first(long),  # Retain the first value of longitude for the city\n#     group = first(group) # Retain the group value from the world map dataset\n#   ) %>%\n#   filter(!is.na(cities_of_authors)) %>% # Remove rows where we can't get the city\n#     arrange(desc(num_papers)) %>%  # Just for easier viewing\n#    # Tidy the behavioural addiction labels:\n#     mutate(Label = str_replace_all(Label, \"_\", \" \") %>%\n#                  str_to_title()) %>%\n#   # Add Categories for behavioural addictions:\n#     mutate(labels_filter = case_when(Label == \"Gambling\" ~ \"Gambling addiction\",\n#                                        Label == \"Gaming\" ~ \"Gaming addiction\",\n#                                        Label == \"Smart Phone\" ~ \"Smart Phone addiction\",\n#                                        Label == \"Exercise\" ~ \"Exercise addiction\",\n#                                        Label == \"Social Media\" ~ \"Social Media addiction\",\n#                                        Label == \"Tanning\" |\n#                                        Label == \"Selfie\" |\n#                                        Label == \"Crime\" |\n#                                        Label == \"Dance\" |\n#                                        Label == \"Joyriding\" |\n#                                        Label == \"Polysurgical\" |\n#                                        Label == \"Death\" |\n#                                          Label == \"Near Death\" |\n#                                          Label == \"Fortune Telling\" |\n#                                          Label == \"Love\"\n#                                          ~ \"The more obscure \\\"addiction\\\"\",\n#                                        TRUE ~ \"REMOVE\")) %>%\n#   filter(labels_filter != \"REMOVE\")  %>%\n#   mutate(labels_filter = as.factor(labels_filter)) %>%\n#   ungroup()\n# \n# \n# \n# # Now we need to create descriptive labels to appear with each addiction:\n# # Before proceeding, let's create a simple dataset to use that contains our  behavioural addiction categories that we can use for grouping:\n# \n# location_grouping_data<- data_locations_with_full_geo_location_WORLD %>% # We need to use the earlier dataset and recreate our grouping labels, as in the dataset we created above we removed papers we couldn't locate them/ get a city\n#   # Add Categories for behavioural addictions:\n#     mutate(labels_filter = case_when(Label == \"Gambling\" ~ \"Gambling addiction\",\n#                                        Label == \"Gaming\" ~ \"Gaming addiction\",\n#                                        Label == \"Smart Phone\" ~ \"Smart Phone addiction\",\n#                                        Label == \"Exercise\" ~ \"Exercise addiction\",\n#                                        Label == \"Social Media\" ~ \"Social Media addiction\",\n#                                        Label == \"Tanning\" |\n#                                        Label == \"Selfie\" |\n#                                        Label == \"Crime\" |\n#                                        Label == \"Dance\" |\n#                                        Label == \"Joyriding\" |\n#                                        Label == \"Polysurgical\" |\n#                                        Label == \"Death\" |\n#                                          Label == \"Near Death\" |\n#                                          Label == \"Fortune Telling\" |\n#                                          Label == \"Love\"\n#                                          ~ \"The more obscure \\\"addictions\\\"\",\n#                                        TRUE ~ \"REMOVE\"))\n# \n# # First create a dataset that tells us the number of **papers per addiction**:\n# location_grouping_data %>%\n#   group_by(labels_filter) %>% \n#   summarise(\n#     papers_per_addiction = n_distinct(PMID),\n#     cities_per_addiction = n_distinct(cities_of_authors),\n#     countries_per_addiction = n_distinct(country.etc)\n#     ) %>%\n#  mutate(description = paste(papers_per_addiction, \"unique papers\"\n#                                 ))\n# \n# \n#    mutate(gambling_addiction = case_when(labels_filter == \"Gambling addiction\" ~ sprintf(\"1961  \\nFirst available study: \\n\\\"Compulsive gambling\\\"\"),\n#                                TRUE ~ \"\")) %>%\n#     \n#      \n# # Get palette colors:\n# num_labels <- length(unique(data_locations_with_full_geo_location_WORLD_city_label_aggregate$labels_filter))\n# palette\n# \n# \n# colors_viridis<- viridis(n = length(unique(data_locations_with_full_geo_location_WORLD_city_label_aggregate$labels_filter)), option = \"plasma\", direction = -1)\n#           \n# labels<- unique(data_locations_with_full_geo_location_WORLD_city_label_aggregate$labels_filter)\n# \n# color_map_df <- tibble(\n#   label = labels,\n#   color = colors_viridis\n# )\n# \n# color_map <- setNames(color_map_df$color, color_map_df$label)\n# \n# \n# # Plot data:\n# filtered_publications_map <- ggplot(world_map, aes(x = long, y = lat, group = group)) +\n#   geom_polygon(fill = \"#289998\", colour = \"#289998\") +\n#   geom_point(data = data_locations_with_full_geo_location_WORLD_city_label_aggregate,\n#              aes(x = long, y = lat,\n#                  size = sqrt(num_papers),\n#                   colour = labels_filter,\n#                  fill = labels_filter),\n#              alpha = 1,\n#              shape = 21) +\n#   # Removal guides\n#   scale_size_continuous(guide = \"none\") +\n#   scale_color_viridis(discrete = TRUE, option = \"plasma\", guide = FALSE, direction = -1)+\n#   scale_fill_viridis(discrete = TRUE, option = \"plasma\", guide = FALSE, direction = -1) +\n#   # scale_fill_manual(values = palette_colors, guide = FALSE) +\n#   # scale_color_manual(values = palette_colors,\n#   #                       # rep(\"#F5F7F0\",num_labels),\n#   #                     guide = FALSE) +\n#   # guides(fill = \"none\", color = \"none\") +\n#   scale_x_continuous(expand = c(0.001, 0.001)) +\n#   scale_y_continuous(expand = c(0.001, 0.001)) +\n#   plot_theme +\n#   theme(\n#     axis.text = element_blank(),\n#     axis.title = element_blank(),\n#      plot.title = element_text(margin = margin(b = 20))\n#     ) +\n#   labs(\n#     title = \"{closest_state} papers\",\n#     caption = \"Rob Heirene (@rheirene)\"\n#   ) +\n#   transition_states( ### NOT FILTER!\n#      labels_filter,\n#      transition_length = 2,\n#      state_length = 6,\n#      wrap = FALSE\n#    ) +\n#   enter_fade() +\n#   exit_fade()\n# \n# \n# \n# # Animate plot in GIF format:\n# animate(filtered_publications_map,\n#         fps = 20,\n#         end_pause = 70,\n#         duration = 30,\n#         width = 800, height = 540,\n#         type = \"cairo\")\n# \n# # The more obscure addictions referred to include turning, selfie, crime, dance, joyriding, polysurgical,  death, near death, fortune telling, and love addictions.\n```\n:::\n\n\n\n\n![](filtered_publications_map.gif){fig-align=\"center\"}\n\n### Where are these published?\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code: Create sankey plot of paper destinations\"}\n# Wrangle data for plot:\nsankey_data<- data %>%\n  distinct(PMID, .keep_all = TRUE) %>% \n    mutate(Label = str_replace_all(Label, \"_\", \" \") %>%\n                 str_to_title()) %>% \n    mutate(Behavioural_addiction = case_when(Label == \"Tanning\" |\n                                       Label == \"Selfie\" |\n                                       Label == \"Crime\" |\n                                       Label == \"Dance\" |\n                                       Label == \"Joyriding\" |\n                                       Label == \"Polysurgical\" |\n                                       Label == \"Death\" |\n                                         Label == \"Near Death\" |\n                                         Label == \"Fortune Telling\" |\n                                         Label == \"Love\"\n                                         ~ \"More obscure \\\"addictions\\\"\",\n                                     TRUE ~ Label))\n```\n:::\n\n\n::: {.callout-tip collapse=\"true\" appearance=\"minimal\" icon=\"false\"}\n## Expand for session information\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code: Get session info\"}\nsession_info(pkgs = \"attached\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16 ucrt)\n os       Windows 10 x64 (build 19045)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United Kingdom.utf8\n ctype    English_United Kingdom.utf8\n tz       Europe/London\n date     2023-09-18\n pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version  date (UTC) lib source\n av          * 0.8.3    2023-02-05 [1] CRAN (R 4.3.0)\n Cairo       * 1.6-0    2022-07-05 [1] CRAN (R 4.3.0)\n dplyr       * 1.1.2    2023-04-20 [1] CRAN (R 4.3.1)\n forcats     * 1.0.0    2023-01-29 [1] CRAN (R 4.3.1)\n gganimate   * 1.0.8    2022-09-08 [1] CRAN (R 4.3.1)\n ggplot2     * 3.4.2    2023-04-03 [1] CRAN (R 4.3.1)\n ggtext      * 0.1.2    2022-09-16 [1] CRAN (R 4.3.1)\n gifski      * 1.12.0-1 2023-06-08 [1] CRAN (R 4.3.0)\n groundhog   * 3.1.0    2023-05-05 [1] CRAN (R 4.3.1)\n gt          * 0.9.0    2023-03-31 [1] CRAN (R 4.3.1)\n gtExtras    * 0.4.5    2022-11-28 [1] CRAN (R 4.3.1)\n htmlwidgets * 1.6.2    2023-03-17 [1] CRAN (R 4.3.1)\n lubridate   * 1.9.2    2023-02-10 [1] CRAN (R 4.3.1)\n maps        * 3.4.1    2022-10-30 [1] CRAN (R 4.3.1)\n plotly      * 4.10.2   2023-06-03 [1] CRAN (R 4.3.1)\n png         * 0.1-8    2022-11-29 [1] CRAN (R 4.3.0)\n purrr       * 1.0.1    2023-01-10 [1] CRAN (R 4.3.1)\n readr       * 2.1.4    2023-02-10 [1] CRAN (R 4.3.1)\n rmarkdown   * 2.23     2023-07-01 [1] CRAN (R 4.3.1)\n scico       * 1.4.0    2023-05-30 [1] CRAN (R 4.3.0)\n sessioninfo * 1.2.2    2021-12-06 [1] CRAN (R 4.3.1)\n showtext    * 0.9-6    2023-05-03 [1] CRAN (R 4.3.1)\n showtextdb  * 3.0      2020-06-04 [1] CRAN (R 4.3.1)\n stringr     * 1.5.0    2022-12-02 [1] CRAN (R 4.3.1)\n sysfonts    * 0.8.8    2022-03-13 [1] CRAN (R 4.3.1)\n tibble      * 3.2.1    2023-03-20 [1] CRAN (R 4.3.1)\n tidyr       * 1.3.0    2023-01-24 [1] CRAN (R 4.3.1)\n tidyverse   * 2.0.0    2023-02-22 [1] CRAN (R 4.3.1)\n transformr  * 0.1.4    2022-08-18 [1] CRAN (R 4.3.1)\n viridis     * 0.6.4    2023-07-22 [1] CRAN (R 4.3.1)\n viridisLite * 0.4.2    2023-05-02 [1] CRAN (R 4.3.1)\n\n [1] C:/Users/rheirene/AppData/Local/R/win-library/4.3\n [2] C:/Program Files/R/R-4.3.1/library\n\n──────────────────────────────────────────────────────────────────────────────\n```\n:::\n:::\n\n:::\n\n------------------------------------------------------------------------\n\n\n```{=html}\n<script src=\"https://giscus.app/client.js\"\n        data-repo=\"rheirene/Quarto_Website\"\n        data-repo-id=\"R_kgDOJ0d4fA\"\n        data-category-id=\"DIC_kwDOJ0d4fM4CXv7I\"\n        data-mapping=\"pathname\"\n        data-strict=\"0\"\n        data-reactions-enabled=\"1\"\n        data-emit-metadata=\"0\"\n        data-input-position=\"bottom\"\n        data-theme=\"dark_dimmed\"\n        data-lang=\"en\"\n        crossorigin=\"anonymous\"\n        async>\n</script\n```\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}